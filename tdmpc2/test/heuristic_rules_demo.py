#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯å‘å¼è§„åˆ™ä½œç”¨æ¼”ç¤ºè„šæœ¬

æœ¬è„šæœ¬æ¼”ç¤ºå¯å‘å¼è§„åˆ™åœ¨åå¥½å­¦ä¹ ç³»ç»Ÿä¸­çš„å…·ä½“ä½œç”¨å’Œè®¡ç®—è¿‡ç¨‹
"""

import sys
import os
sys.path.append('/public/home/yaotianxiao2024/SPE')

# ç¡®ä¿prm.apiæ¨¡å—å¯ä»¥è¢«æ­£ç¡®å¯¼å…¥
sys.path.append('/public/home/yaotianxiao2024/SPE/prm')

import numpy as np
from prm.preference_labeling_engine import PreferenceLabelingEngine
from prm.trajectory_metrics import TrajectoryQualityEvaluator
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_demo_trajectory(length=100, reward_level='high', quality_level='high'):
    """
    åˆ›å»ºæ¼”ç¤ºç”¨çš„è½¨è¿¹æ•°æ®
    
    Args:
        length: è½¨è¿¹é•¿åº¦
        reward_level: å¥–åŠ±æ°´å¹³ ('high', 'medium', 'low')
        quality_level: æ‰§è¡Œè´¨é‡ ('high', 'medium', 'low')
    """
    
    # æ ¹æ®å¥–åŠ±æ°´å¹³è®¾ç½®ç¯å¢ƒå¥–åŠ±
    if reward_level == 'high':
        base_reward = 2.0
        reward_noise = 0.3
    elif reward_level == 'medium':
        base_reward = 1.0
        reward_noise = 0.2
    else:  # low
        base_reward = 0.2
        reward_noise = 0.1
    
    # æ ¹æ®è´¨é‡æ°´å¹³è®¾ç½®åŠ¨ä½œå’ŒçŠ¶æ€ç‰¹å¾
    if quality_level == 'high':
        action_noise = 0.1
        state_noise = 0.05
        survival_factor = 1.0
    elif quality_level == 'medium':
        action_noise = 0.3
        state_noise = 0.15
        survival_factor = 0.8
    else:  # low
        action_noise = 0.8
        state_noise = 0.4
        survival_factor = 0.6
    
    # è°ƒæ•´å®é™…é•¿åº¦
    actual_length = int(length * survival_factor)
    
    # ç”Ÿæˆè½¨è¿¹æ•°æ®
    observations = []
    actions = []
    rewards = []
    
    for i in range(actual_length):
        # ç”Ÿæˆè§‚æµ‹ï¼ˆæ¨¡æ‹Ÿæœºå™¨äººçŠ¶æ€ï¼‰
        obs = np.random.randn(37) * state_noise  # H1æœºå™¨äººé€šå¸¸æœ‰37ç»´çŠ¶æ€
        obs[2] = 1.0 + np.random.randn() * 0.1  # å¤´éƒ¨é«˜åº¦
        observations.append(obs)
        
        # ç”ŸæˆåŠ¨ä½œ
        action = np.random.randn(19) * action_noise  # H1æœºå™¨äººé€šå¸¸æœ‰19ç»´åŠ¨ä½œ
        actions.append(action)
        
        # ç”Ÿæˆå¥–åŠ±
        reward = base_reward + np.random.randn() * reward_noise
        rewards.append(reward)
    
    return {
        'observations': observations,
        'actions': actions,
        'rewards': rewards,
        'obs': observations,  # å…¼å®¹æ€§åˆ«å
        'action': actions,    # å…¼å®¹æ€§åˆ«å
        'reward': rewards     # å…¼å®¹æ€§åˆ«å
    }

def demonstrate_api_rules_loading():
    """
    æ¼”ç¤ºAPIè§„åˆ™çš„åŠ è½½è¿‡ç¨‹
    """
    print("\n" + "="*80)
    print("ğŸ”§ APIè§„åˆ™åŠ è½½æ¼”ç¤º")
    print("="*80)
    
    # åˆ›å»ºåå¥½æ ‡æ³¨å¼•æ“
    engine = PreferenceLabelingEngine()
    
    # æ£€æŸ¥APIè§„åˆ™åŠ è½½æƒ…å†µ
    if hasattr(engine, 'api_rules') and engine.api_rules:
        print(f"âœ… æˆåŠŸåŠ è½½ {len(engine.api_rules)} ä¸ªAPIè§„åˆ™å‡½æ•°:")
        for rule_name in engine.api_rules.keys():
            print(f"   - {rule_name}")
    else:
        print("âŒ æœªåŠ è½½ä»»ä½•APIè§„åˆ™")
    
    return engine

def demonstrate_trajectory_quality_evaluation(engine):
    """
    æ¼”ç¤ºè½¨è¿¹è´¨é‡è¯„ä¼°ä¸­å¯å‘å¼è§„åˆ™çš„ä½œç”¨
    """
    print("\n" + "="*80)
    print("ğŸ“Š è½¨è¿¹è´¨é‡è¯„ä¼°ä¸­çš„å¯å‘å¼è§„åˆ™ä½œç”¨")
    print("="*80)
    
    # åˆ›å»ºä¸åŒç±»å‹çš„è½¨è¿¹
    trajectories = {
        "é«˜å¥–åŠ±é«˜è´¨é‡": create_demo_trajectory(150, 'high', 'high'),
        "é«˜å¥–åŠ±ä½è´¨é‡": create_demo_trajectory(150, 'high', 'low'),
        "ä½å¥–åŠ±é«˜è´¨é‡": create_demo_trajectory(150, 'low', 'high'),
        "ä½å¥–åŠ±ä½è´¨é‡": create_demo_trajectory(150, 'low', 'low')
    }
    
    print("\nğŸ§ª è½¨è¿¹è´¨é‡è¯„ä¼°ç»“æœ:")
    
    for traj_name, traj_data in trajectories.items():
        try:
            # è®¡ç®—ç¯å¢ƒå¥–åŠ±æ€»å’Œ
            env_reward_sum = sum(traj_data['rewards'])
            
            # ä½¿ç”¨è´¨é‡è¯„ä¼°å™¨è¯„ä¼°è½¨è¿¹
            quality_score, detailed_scores = engine.quality_evaluator.evaluate_trajectory_quality(
                traj_data['observations'], 
                traj_data['actions'], 
                traj_data['rewards']
            )
            
            print(f"\nğŸ“ˆ {traj_name}:")
            print(f"   ç¯å¢ƒå¥–åŠ±æ€»å’Œ: {env_reward_sum:.2f}")
            print(f"   è´¨é‡åˆ†æ•°: {quality_score:.4f}")
            print(f"   è¯¦ç»†åˆ†æ•°: {detailed_scores}")
            
            # åˆ†æå¯å‘å¼è§„åˆ™çš„è´¡çŒ®
            if hasattr(engine.quality_evaluator, '_apply_api_rules'):
                # å°è¯•è·å–APIè§„åˆ™çš„è´¡çŒ®
                try:
                    api_bonus = engine.quality_evaluator._apply_api_rules(
                        np.array(traj_data['observations']),
                        np.array(traj_data['actions']),
                        detailed_scores,
                        {}
                    )
                    print(f"   APIè§„åˆ™è´¡çŒ®: {api_bonus:.4f}")
                except Exception as e:
                    print(f"   APIè§„åˆ™è´¡çŒ®: æ— æ³•è®¡ç®— ({e})")
            
        except Exception as e:
            print(f"âŒ {traj_name} è¯„ä¼°å¤±è´¥: {e}")

def demonstrate_preference_calculation(engine):
    """
    æ¼”ç¤ºåå¥½è®¡ç®—ä¸­å¯å‘å¼è§„åˆ™çš„ä½œç”¨
    """
    print("\n" + "="*80)
    print("ğŸ¯ åå¥½è®¡ç®—ä¸­çš„å¯å‘å¼è§„åˆ™ä½œç”¨")
    print("="*80)
    
    # åˆ›å»ºå¯¹æ¯”è½¨è¿¹å¯¹
    traj_a = create_demo_trajectory(120, 'high', 'high')  # é«˜å¥–åŠ±é«˜è´¨é‡
    traj_b = create_demo_trajectory(100, 'medium', 'low')  # ä¸­ç­‰å¥–åŠ±ä½è´¨é‡
    
    print("\nğŸ”„ è½¨è¿¹å¯¹æ¯”åˆ†æ:")
    
    # è®¡ç®—ç¯å¢ƒå¥–åŠ±
    reward_a = sum(traj_a['rewards'])
    reward_b = sum(traj_b['rewards'])
    
    print(f"è½¨è¿¹A - ç¯å¢ƒå¥–åŠ±: {reward_a:.2f}, é•¿åº¦: {len(traj_a['observations'])}")
    print(f"è½¨è¿¹B - ç¯å¢ƒå¥–åŠ±: {reward_b:.2f}, é•¿åº¦: {len(traj_b['observations'])}")
    
    try:
        # ä½¿ç”¨åå¥½æ ‡æ³¨å¼•æ“è®¡ç®—åå¥½
        preference_result = engine.generate_preference_label(
            np.array(traj_a['observations']),
            np.array(traj_a['actions']),
            np.array(traj_b['observations']),
            np.array(traj_b['actions']),
            trajectory_a_data=traj_a,
            trajectory_b_data=traj_b
        )
        
        if preference_result:
            preference_score, confidence, label_type = preference_result
            print(f"\nğŸ“Š åå¥½è®¡ç®—ç»“æœ:")
            print(f"   åå¥½åˆ†æ•°: {preference_score:.4f} (>0.5åå¥½A, <0.5åå¥½B)")
            print(f"   ç½®ä¿¡åº¦: {confidence:.4f}")
            print(f"   æ ‡ç­¾ç±»å‹: {label_type}")
            
            # è§£é‡Šç»“æœ
            if preference_score > 0.6:
                print(f"   âœ… å¼ºåå¥½è½¨è¿¹A (ç½®ä¿¡åº¦: {confidence*100:.1f}%)")
            elif preference_score < 0.4:
                print(f"   âœ… å¼ºåå¥½è½¨è¿¹B (ç½®ä¿¡åº¦: {confidence*100:.1f}%)")
            else:
                print(f"   âš–ï¸ åå¥½ä¸æ˜ç¡® (ç½®ä¿¡åº¦: {confidence*100:.1f}%)")
        else:
            print("âŒ åå¥½è®¡ç®—å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ åå¥½è®¡ç®—å‡ºé”™: {e}")

def demonstrate_heuristic_vs_environment_reward():
    """
    æ¼”ç¤ºå¯å‘å¼è¯„ä¼°ä¸ç¯å¢ƒå¥–åŠ±çš„å¯¹æ¯”
    """
    print("\n" + "="*80)
    print("âš–ï¸ å¯å‘å¼è¯„ä¼° vs ç¯å¢ƒå¥–åŠ±å¯¹æ¯”")
    print("="*80)
    
    # åˆ›å»ºé—®é¢˜åœºæ™¯ï¼šé«˜ç¯å¢ƒå¥–åŠ±ä½†åŠ¨ä½œæ¿€è¿›çš„è½¨è¿¹
    print("\nğŸ§ª é—®é¢˜åœºæ™¯åˆ†æ:")
    
    scenarios = [
        {
            "name": "é«˜å¥–åŠ±æ¿€è¿›åŠ¨ä½œ",
            "description": "ä»»åŠ¡æˆåŠŸä½†åŠ¨ä½œå˜åŒ–å¤§",
            "reward_level": "high",
            "quality_level": "low",
            "expected": "åº”è¯¥è·å¾—æ­£å‘åå¥½"
        },
        {
            "name": "ä½å¥–åŠ±å¹³æ»‘åŠ¨ä½œ", 
            "description": "ä»»åŠ¡å¤±è´¥ä½†åŠ¨ä½œå¹³æ»‘",
            "reward_level": "low",
            "quality_level": "high",
            "expected": "åº”è¯¥è·å¾—è´Ÿå‘åå¥½"
        },
        {
            "name": "é«˜å¥–åŠ±é«˜è´¨é‡",
            "description": "ç†æƒ³æƒ…å†µ",
            "reward_level": "high",
            "quality_level": "high",
            "expected": "åº”è¯¥è·å¾—å¼ºæ­£å‘åå¥½"
        }
    ]
    
    engine = PreferenceLabelingEngine()
    
    for scenario in scenarios:
        print(f"\nğŸ“‹ {scenario['name']} - {scenario['description']}")
        
        # åˆ›å»ºè½¨è¿¹
        traj = create_demo_trajectory(
            120, 
            scenario['reward_level'], 
            scenario['quality_level']
        )
        
        # è®¡ç®—ç¯å¢ƒå¥–åŠ±
        env_reward = sum(traj['rewards'])
        
        try:
            # è®¡ç®—è´¨é‡åˆ†æ•°ï¼ˆåŒ…å«å¯å‘å¼è§„åˆ™ï¼‰
            quality_score, detailed_scores = engine.quality_evaluator.evaluate_trajectory_quality(
                traj['observations'],
                traj['actions'],
                traj['rewards']
            )
            
            print(f"   ğŸŒ ç¯å¢ƒå¥–åŠ±æ€»å’Œ: {env_reward:.2f}")
            print(f"   ğŸ§  è´¨é‡åˆ†æ•°: {quality_score:.4f}")
            print(f"   ğŸ“ {scenario['expected']}")
            
            # åˆ†æä¸€è‡´æ€§
            env_normalized = env_reward / 100.0  # ç®€å•å½’ä¸€åŒ–
            if (env_normalized > 0.5 and quality_score > env_normalized * 0.8) or \
               (env_normalized <= 0.5 and quality_score <= env_normalized * 1.2):
                print(f"   âœ… å¯å‘å¼è¯„ä¼°ä¸ç¯å¢ƒå¥–åŠ±åŸºæœ¬ä¸€è‡´")
            else:
                print(f"   âš ï¸ å¯å‘å¼è¯„ä¼°ä¸ç¯å¢ƒå¥–åŠ±å­˜åœ¨å·®å¼‚")
                
        except Exception as e:
            print(f"   âŒ è¯„ä¼°å¤±è´¥: {e}")

def main():
    """
    ä¸»æ¼”ç¤ºå‡½æ•°
    """
    print("ğŸš€ å¯å‘å¼è§„åˆ™ä½œç”¨æ¼”ç¤º")
    print("æœ¬æ¼”ç¤ºå°†å±•ç¤ºå¯å‘å¼è§„åˆ™åœ¨åå¥½å­¦ä¹ ç³»ç»Ÿä¸­çš„å…·ä½“ä½œç”¨å’Œè®¡ç®—è¿‡ç¨‹")
    
    try:
        # 1. æ¼”ç¤ºAPIè§„åˆ™åŠ è½½
        engine = demonstrate_api_rules_loading()
        
        # 2. æ¼”ç¤ºè½¨è¿¹è´¨é‡è¯„ä¼°
        demonstrate_trajectory_quality_evaluation(engine)
        
        # 3. æ¼”ç¤ºåå¥½è®¡ç®—
        demonstrate_preference_calculation(engine)
        
        # 4. æ¼”ç¤ºå¯å‘å¼è¯„ä¼°ä¸ç¯å¢ƒå¥–åŠ±å¯¹æ¯”
        demonstrate_heuristic_vs_environment_reward()
        
        print("\n" + "="*80)
        print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("="*80)
        print("\nğŸ“š æ€»ç»“:")
        print("1. å¯å‘å¼è§„åˆ™é€šè¿‡APIæ–‡ä»¶ä¸ºç‰¹å®šä»»åŠ¡æä¾›ä¸“ç”¨è¯„ä¼°é€»è¾‘")
        print("2. åœ¨è½¨è¿¹è´¨é‡è¯„ä¼°ä¸­ï¼Œå¯å‘å¼è§„åˆ™ä½œä¸ºé¢å¤–çš„è¯„ä¼°ç»´åº¦å‚ä¸è®¡ç®—")
        print("3. åœ¨åå¥½è®¡ç®—ä¸­ï¼Œå¯å‘å¼è§„åˆ™é€šè¿‡è´¨é‡è¯„ä¼°é—´æ¥å½±å“åå¥½åˆ¤æ–­")
        print("4. å½“å‰ç³»ç»Ÿå·²ä¼˜åŒ–ï¼Œç¡®ä¿å¯å‘å¼è¯„ä¼°ä¸ç¯å¢ƒå¥–åŠ±çš„ä¸€è‡´æ€§")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()