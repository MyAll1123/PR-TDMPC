#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åå¥½å¥–åŠ±æ¨¡å‹é›†æˆæµ‹è¯•

æµ‹è¯•ä¿®æ”¹åçš„åå¥½å¥–åŠ±æ¨¡å‹åœ¨å®é™…TD-MPC2è®­ç»ƒç¯å¢ƒä¸­çš„é›†æˆæƒ…å†µ

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-08-29
"""

import sys
import os
sys.path.append('/public/home/yaotianxiao2024/SPE')

import torch
import numpy as np
from tdmpc2.common.buffer import Buffer
from tdmpc2.common.world_model import WorldModel
from tdmpc2.common.scale import RunningScale
from prm.hybrid_value_estimator import HybridValueEstimator
from prm.optimized_latent_preference_model import (
    OptimizedLatentPreferenceModel, 
    OptimizedLatentPreferenceConfig,
    create_optimized_latent_preference_model
)

def test_hybrid_value_estimator_integration():
    """æµ‹è¯•HybridValueEstimatorä¸ä¿®æ”¹åçš„åå¥½å¥–åŠ±æ¨¡å‹çš„é›†æˆ"""
    print("=" * 60)
    print("HybridValueEstimator é›†æˆæµ‹è¯•")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿé…ç½®
    class MockConfig:
        def __init__(self):
            self.obs_shape = (84, 84, 3)
            self.action_dim = 61
            self.latent_dim = 512
            self.hidden_dim = 256
            self.horizon = 5
            self.discount = 0.99
            self.device = 'cpu'
            self.preference_integration_method = 'multiplicative'
            self.environment_weight = 0.7
            self.preference_weight = 0.3
            self.enable_uncertainty = True
    
    config = MockConfig()
    
    # åˆ›å»ºåå¥½å¥–åŠ±æ¨¡å‹
    pref_config = OptimizedLatentPreferenceConfig(
        latent_dim=config.latent_dim,
        action_dim=config.action_dim,
        hidden_dim=config.hidden_dim,
        enable_uncertainty=config.enable_uncertainty
    )
    
    preference_model = create_optimized_latent_preference_model(pref_config)
    
    # æ¨¡æ‹Ÿä¸–ç•Œæ¨¡å‹
    class MockWorldModel:
        def __init__(self):
            self.device = config.device
        
        def next(self, z, a):
            # è¿”å›ä¸‹ä¸€ä¸ªæ½œåœ¨çŠ¶æ€å’Œå¥–åŠ±
            next_z = torch.randn_like(z)
            reward = torch.randn(z.shape[0])
            return next_z, reward
        
        def reward(self, z, a):
            return torch.randn(z.shape[0])
    
    world_model = MockWorldModel()
    
    # åˆ›å»ºHybridValueEstimator
    try:
        hybrid_estimator = HybridValueEstimator(
            world_model=world_model,
            preference_model=preference_model,
            config=config
        )
        print("âœ… HybridValueEstimator åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ HybridValueEstimator åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•ä»·å€¼ä¼°è®¡
    print("\næµ‹è¯•ä»·å€¼ä¼°è®¡åŠŸèƒ½...")
    
    batch_size = 4
    z = torch.randn(batch_size, config.latent_dim)
    a = torch.randn(batch_size, config.action_dim)
    
    try:
        # æµ‹è¯•å•æ­¥ä»·å€¼ä¼°è®¡
        value = hybrid_estimator._estimate_value(z, a)
        print(f"âœ… å•æ­¥ä»·å€¼ä¼°è®¡æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {value.shape}")
        print(f"   ä»·å€¼èŒƒå›´: [{value.min().item():.3f}, {value.max().item():.3f}]")
        
        # æµ‹è¯•å¤šæ­¥ä»·å€¼ä¼°è®¡
        actions = torch.randn(batch_size, config.horizon, config.action_dim)
        total_value = hybrid_estimator.estimate_value(z, actions)
        print(f"âœ… å¤šæ­¥ä»·å€¼ä¼°è®¡æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {total_value.shape}")
        print(f"   æ€»ä»·å€¼èŒƒå›´: [{total_value.min().item():.3f}, {total_value.max().item():.3f}]")
        
    except Exception as e:
        print(f"âŒ ä»·å€¼ä¼°è®¡å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•åå¥½å¥–åŠ±é›†æˆ
    print("\næµ‹è¯•åå¥½å¥–åŠ±é›†æˆ...")
    
    try:
        # è·å–ç¯å¢ƒå¥–åŠ±å’Œåå¥½å¥–åŠ±
        env_reward = world_model.reward(z, a)
        pref_reward, confidence = preference_model.get_preference_reward(z[0], a[0])
        
        print(f"ç¯å¢ƒå¥–åŠ±ç¤ºä¾‹: {env_reward[0].item():.3f}")
        print(f"åå¥½å¥–åŠ±ç¤ºä¾‹: {pref_reward:.3f} (ç½®ä¿¡åº¦: {confidence:.3f})")
        
        # éªŒè¯åå¥½å¥–åŠ±èŒƒå›´
        if pref_reward > 0:
            if 0.1 <= pref_reward <= 0.3:
                print("âœ… æ­£åå¥½å¥–åŠ±åœ¨æ­£ç¡®èŒƒå›´å†… [0.1, 0.3]")
            else:
                print(f"âŒ æ­£åå¥½å¥–åŠ±è¶…å‡ºèŒƒå›´: {pref_reward}")
        elif pref_reward < 0:
            if -0.3 <= pref_reward <= -0.1:
                print("âœ… è´Ÿåå¥½å¥–åŠ±åœ¨æ­£ç¡®èŒƒå›´å†… [-0.3, -0.1]")
            else:
                print(f"âŒ è´Ÿåå¥½å¥–åŠ±è¶…å‡ºèŒƒå›´: {pref_reward}")
        else:
            print("âš ï¸  åå¥½å¥–åŠ±ä¸ºé›¶")
        
    except Exception as e:
        print(f"âŒ åå¥½å¥–åŠ±é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print("\nâœ… æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼")
    return True

def test_preference_reward_statistics():
    """æµ‹è¯•åå¥½å¥–åŠ±çš„ç»Ÿè®¡ç‰¹æ€§"""
    print("\n" + "=" * 60)
    print("åå¥½å¥–åŠ±ç»Ÿè®¡ç‰¹æ€§æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºåå¥½å¥–åŠ±æ¨¡å‹
    config = OptimizedLatentPreferenceConfig(
        latent_dim=512,
        action_dim=61,
        hidden_dim=256,
        enable_uncertainty=True
    )
    
    model = create_optimized_latent_preference_model(config)
    
    # æ”¶é›†å¤§é‡æ ·æœ¬
    rewards = []
    confidences = []
    
    print("æ”¶é›†1000ä¸ªæ ·æœ¬è¿›è¡Œç»Ÿè®¡åˆ†æ...")
    
    for i in range(1000):
        latent_state = torch.randn(512)
        action = torch.randn(61)
        reward, confidence = model.get_preference_reward(latent_state, action)
        rewards.append(reward)
        confidences.append(confidence)
    
    rewards = np.array(rewards)
    confidences = np.array(confidences)
    
    # ç»Ÿè®¡åˆ†æ
    print(f"\nå¥–åŠ±ç»Ÿè®¡:")
    print(f"  èŒƒå›´: [{rewards.min():.3f}, {rewards.max():.3f}]")
    print(f"  å‡å€¼: {rewards.mean():.3f}")
    print(f"  æ ‡å‡†å·®: {rewards.std():.3f}")
    
    print(f"\nç½®ä¿¡åº¦ç»Ÿè®¡:")
    print(f"  èŒƒå›´: [{confidences.min():.3f}, {confidences.max():.3f}]")
    print(f"  å‡å€¼: {confidences.mean():.3f}")
    print(f"  æ ‡å‡†å·®: {confidences.std():.3f}")
    
    # åˆ†æä¸åŒç½®ä¿¡åº¦åŒºé—´çš„å¥–åŠ±åˆ†å¸ƒ
    high_conf_mask = confidences >= 0.7
    mid_conf_mask = (confidences > 0.4) & (confidences < 0.7)
    low_conf_mask = confidences <= 0.4
    
    print(f"\nä¸åŒç½®ä¿¡åº¦åŒºé—´çš„å¥–åŠ±åˆ†å¸ƒ:")
    
    if np.any(high_conf_mask):
        high_rewards = rewards[high_conf_mask]
        print(f"  é«˜ç½®ä¿¡åº¦ (>=0.7): {np.sum(high_conf_mask)} æ ·æœ¬")
        print(f"    å¥–åŠ±èŒƒå›´: [{high_rewards.min():.3f}, {high_rewards.max():.3f}]")
        print(f"    å¥–åŠ±å‡å€¼: {high_rewards.mean():.3f}")
    
    if np.any(mid_conf_mask):
        mid_rewards = rewards[mid_conf_mask]
        print(f"  ä¸­ç­‰ç½®ä¿¡åº¦ (0.4, 0.7): {np.sum(mid_conf_mask)} æ ·æœ¬")
        print(f"    å¥–åŠ±èŒƒå›´: [{mid_rewards.min():.3f}, {mid_rewards.max():.3f}]")
        print(f"    å¥–åŠ±å‡å€¼: {mid_rewards.mean():.3f}")
    
    if np.any(low_conf_mask):
        low_rewards = rewards[low_conf_mask]
        print(f"  ä½ç½®ä¿¡åº¦ (<=0.4): {np.sum(low_conf_mask)} æ ·æœ¬")
        print(f"    å¥–åŠ±èŒƒå›´: [{low_rewards.min():.3f}, {low_rewards.max():.3f}]")
        print(f"    å¥–åŠ±å‡å€¼: {low_rewards.mean():.3f}")
    
    # éªŒè¯çº¦æŸ
    positive_rewards = rewards[rewards > 0]
    negative_rewards = rewards[rewards < 0]
    
    constraints_ok = True
    
    if len(positive_rewards) > 0:
        if not (0.1 <= positive_rewards.min() and positive_rewards.max() <= 0.4):
            print(f"âŒ æ­£å¥–åŠ±èŒƒå›´è¿åçº¦æŸ: [{positive_rewards.min():.3f}, {positive_rewards.max():.3f}]")
            constraints_ok = False
        else:
            print(f"âœ… æ­£å¥–åŠ±èŒƒå›´ç¬¦åˆçº¦æŸ: [{positive_rewards.min():.3f}, {positive_rewards.max():.3f}]")
    
    if len(negative_rewards) > 0:
        if not (-0.3 <= negative_rewards.min() and negative_rewards.max() <= -0.1):
            print(f"âŒ è´Ÿå¥–åŠ±èŒƒå›´è¿åçº¦æŸ: [{negative_rewards.min():.3f}, {negative_rewards.max():.3f}]")
            constraints_ok = False
        else:
            print(f"âœ… è´Ÿå¥–åŠ±èŒƒå›´ç¬¦åˆçº¦æŸ: [{negative_rewards.min():.3f}, {negative_rewards.max():.3f}]")
    
    return constraints_ok

if __name__ == "__main__":
    print("å¼€å§‹åå¥½å¥–åŠ±æ¨¡å‹é›†æˆæµ‹è¯•...")
    
    # è¿è¡Œæµ‹è¯•
    integration_passed = test_hybrid_value_estimator_integration()
    statistics_passed = test_preference_reward_statistics()
    
    print("\n" + "=" * 60)
    print("é›†æˆæµ‹è¯•æ€»ç»“")
    print("=" * 60)
    print(f"HybridValueEstimatoré›†æˆ: {'âœ… é€šè¿‡' if integration_passed else 'âŒ å¤±è´¥'}")
    print(f"ç»Ÿè®¡ç‰¹æ€§éªŒè¯: {'âœ… é€šè¿‡' if statistics_passed else 'âŒ å¤±è´¥'}")
    
    if integration_passed and statistics_passed:
        print("\nğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼åå¥½å¥–åŠ±æ¨¡å‹å¯ä»¥æ­£å¸¸é›†æˆåˆ°TD-MPC2ä¸­ã€‚")
    else:
        print("\nâŒ éƒ¨åˆ†é›†æˆæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")
    
    print("=" * 60)