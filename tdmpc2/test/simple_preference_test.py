#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„åå¥½å¥–åŠ±æ¨¡å‹æµ‹è¯•

é¿å…å¯¼å…¥å¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜çš„æ¨¡å—ï¼Œåªæµ‹è¯•åå¥½å¥–åŠ±æ¨¡å‹çš„æ ¸å¿ƒåŠŸèƒ½

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-08-29
"""

import sys
import os
sys.path.append('/public/home/yaotianxiao2024/SPE')

import torch
import numpy as np

# åªå¯¼å…¥å¿…è¦çš„åå¥½å¥–åŠ±æ¨¡å‹
try:
    from prm.optimized_latent_preference_model import (
        OptimizedLatentPreferenceModel, 
        OptimizedLatentPreferenceConfig,
        create_optimized_latent_preference_model
    )
    print("âœ… æˆåŠŸå¯¼å…¥åå¥½å¥–åŠ±æ¨¡å‹")
except Exception as e:
    print(f"âŒ å¯¼å…¥åå¥½å¥–åŠ±æ¨¡å‹å¤±è´¥: {e}")
    sys.exit(1)

def test_preference_model_functionality():
    """æµ‹è¯•åå¥½å¥–åŠ±æ¨¡å‹çš„æ ¸å¿ƒåŠŸèƒ½"""
    print("=" * 60)
    print("åå¥½å¥–åŠ±æ¨¡å‹æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºé…ç½®
    config = OptimizedLatentPreferenceConfig(
        latent_dim=512,
        action_dim=61,
        hidden_dim=256,
        enable_uncertainty=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    try:
        model = create_optimized_latent_preference_model(config)
        print("âœ… åå¥½å¥–åŠ±æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ åå¥½å¥–åŠ±æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•æ¨¡å‹æ–¹æ³•
    print("\næµ‹è¯•æ¨¡å‹æ–¹æ³•...")
    
    # æµ‹è¯• _map_reward_with_confidence æ–¹æ³•
    try:
        test_cases = [
            (1.0, 0.8, 0.3),    # é«˜ç½®ä¿¡åº¦æ­£å¥–åŠ±
            (-1.0, 0.8, -0.3),  # é«˜ç½®ä¿¡åº¦è´Ÿå¥–åŠ±
            (1.0, 0.2, 0.1),    # ä½ç½®ä¿¡åº¦æ­£å¥–åŠ±
            (-1.0, 0.2, -0.1),  # ä½ç½®ä¿¡åº¦è´Ÿå¥–åŠ±
            (1.0, 0.5, 0.2),    # ä¸­ç­‰ç½®ä¿¡åº¦æ­£å¥–åŠ±
            (-1.0, 0.5, -0.2),  # ä¸­ç­‰ç½®ä¿¡åº¦è´Ÿå¥–åŠ±
        ]
        
        print("æµ‹è¯• _map_reward_with_confidence æ–¹æ³•:")
        all_passed = True
        
        for raw_reward, confidence, expected_approx in test_cases:
            mapped_reward = model._map_reward_with_confidence(raw_reward, confidence)
            print(f"  åŸå§‹å¥–åŠ±: {raw_reward:5.1f}, ç½®ä¿¡åº¦: {confidence:4.1f} -> æ˜ å°„å¥–åŠ±: {mapped_reward:6.3f} (æœŸæœ›çº¦: {expected_approx:5.1f})")
            
            # éªŒè¯èŒƒå›´çº¦æŸ
            if mapped_reward > 0:
                if not (0.1 <= mapped_reward <= 0.4):
                    print(f"    âŒ æ­£å¥–åŠ±è¶…å‡ºèŒƒå›´ [0.1, 0.4]: {mapped_reward}")
                    all_passed = False
            elif mapped_reward < 0:
                if not (-0.4 <= mapped_reward <= -0.1):
                    print(f"    âŒ è´Ÿå¥–åŠ±è¶…å‡ºèŒƒå›´ [-0.4, -0.1]: {mapped_reward}")
                    all_passed = False
        
        if all_passed:
            print("  âœ… _map_reward_with_confidence æ–¹æ³•æµ‹è¯•é€šè¿‡")
        else:
            print("  âŒ _map_reward_with_confidence æ–¹æ³•æµ‹è¯•å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ _map_reward_with_confidence æ–¹æ³•æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯• get_preference_reward æ–¹æ³•
    try:
        print("\næµ‹è¯• get_preference_reward æ–¹æ³•:")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        latent_state = torch.randn(512)
        action = torch.randn(61)
        
        # è°ƒç”¨æ–¹æ³•
        reward, confidence = model.get_preference_reward(latent_state, action)
        
        print(f"  è¾“å…¥å½¢çŠ¶: æ½œåœ¨çŠ¶æ€ {latent_state.shape}, åŠ¨ä½œ {action.shape}")
        print(f"  è¾“å‡º: å¥–åŠ± {reward:.3f}, ç½®ä¿¡åº¦ {confidence:.3f}")
        
        # éªŒè¯è¾“å‡ºç±»å‹å’ŒèŒƒå›´
        if not isinstance(reward, (float, int)):
            print(f"  âŒ å¥–åŠ±ç±»å‹é”™è¯¯: {type(reward)}")
            return False
        
        if not isinstance(confidence, (float, int)):
            print(f"  âŒ ç½®ä¿¡åº¦ç±»å‹é”™è¯¯: {type(confidence)}")
            return False
        
        if not (0.0 <= confidence <= 1.0):
            print(f"  âŒ ç½®ä¿¡åº¦è¶…å‡ºèŒƒå›´ [0, 1]: {confidence}")
            return False
        
        # éªŒè¯å¥–åŠ±èŒƒå›´
        if reward > 0:
            if not (0.1 <= reward <= 0.4):
                print(f"  âŒ æ­£å¥–åŠ±è¶…å‡ºèŒƒå›´ [0.1, 0.4]: {reward}")
                return False
        elif reward < 0:
            if not (-0.4 <= reward <= -0.1):
                print(f"  âŒ è´Ÿå¥–åŠ±è¶…å‡ºèŒƒå›´ [-0.4, -0.1]: {reward}")
                return False
        
        print("  âœ… get_preference_reward æ–¹æ³•æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        print(f"âŒ get_preference_reward æ–¹æ³•æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True

def test_batch_processing():
    """æµ‹è¯•æ‰¹é‡å¤„ç†åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æ‰¹é‡å¤„ç†æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    config = OptimizedLatentPreferenceConfig(
        latent_dim=512,
        action_dim=61,
        hidden_dim=256,
        enable_uncertainty=True
    )
    
    model = create_optimized_latent_preference_model(config)
    
    # æµ‹è¯•å¤šä¸ªæ ·æœ¬
    print("æµ‹è¯•100ä¸ªéšæœºæ ·æœ¬...")
    
    rewards = []
    confidences = []
    
    try:
        for i in range(100):
            latent_state = torch.randn(512)
            action = torch.randn(61)
            reward, confidence = model.get_preference_reward(latent_state, action)
            rewards.append(reward)
            confidences.append(confidence)
        
        rewards = np.array(rewards)
        confidences = np.array(confidences)
        
        print(f"å¥–åŠ±ç»Ÿè®¡:")
        print(f"  èŒƒå›´: [{rewards.min():.3f}, {rewards.max():.3f}]")
        print(f"  å‡å€¼: {rewards.mean():.3f}")
        print(f"  æ ‡å‡†å·®: {rewards.std():.3f}")
        
        print(f"ç½®ä¿¡åº¦ç»Ÿè®¡:")
        print(f"  èŒƒå›´: [{confidences.min():.3f}, {confidences.max():.3f}]")
        print(f"  å‡å€¼: {confidences.mean():.3f}")
        print(f"  æ ‡å‡†å·®: {confidences.std():.3f}")
        
        # éªŒè¯çº¦æŸ
        positive_rewards = rewards[rewards > 0]
        negative_rewards = rewards[rewards < 0]
        
        constraints_ok = True
        
        if len(positive_rewards) > 0:
            if not (0.1 <= positive_rewards.min() and positive_rewards.max() <= 0.4):
                print(f"âŒ æ­£å¥–åŠ±èŒƒå›´è¿åçº¦æŸ: [{positive_rewards.min():.3f}, {positive_rewards.max():.3f}]")
                constraints_ok = False
            else:
                print(f"âœ… æ­£å¥–åŠ±èŒƒå›´ç¬¦åˆçº¦æŸ: [{positive_rewards.min():.3f}, {positive_rewards.max():.3f}]")
        
        if len(negative_rewards) > 0:
            if not (-0.4 <= negative_rewards.min() and negative_rewards.max() <= -0.1):
                print(f"âŒ è´Ÿå¥–åŠ±èŒƒå›´è¿åçº¦æŸ: [{negative_rewards.min():.3f}, {negative_rewards.max():.3f}]")
                constraints_ok = False
            else:
                print(f"âœ… è´Ÿå¥–åŠ±èŒƒå›´ç¬¦åˆçº¦æŸ: [{negative_rewards.min():.3f}, {negative_rewards.max():.3f}]")
        
        if not (0.0 <= confidences.min() and confidences.max() <= 1.0):
            print(f"âŒ ç½®ä¿¡åº¦èŒƒå›´è¿åçº¦æŸ: [{confidences.min():.3f}, {confidences.max():.3f}]")
            constraints_ok = False
        else:
            print(f"âœ… ç½®ä¿¡åº¦èŒƒå›´ç¬¦åˆçº¦æŸ: [{confidences.min():.3f}, {confidences.max():.3f}]")
        
        return constraints_ok
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_confidence_mapping_consistency():
    """æµ‹è¯•ç½®ä¿¡åº¦æ˜ å°„çš„ä¸€è‡´æ€§"""
    print("\n" + "=" * 60)
    print("ç½®ä¿¡åº¦æ˜ å°„ä¸€è‡´æ€§æµ‹è¯•")
    print("=" * 60)
    
    config = OptimizedLatentPreferenceConfig(
        latent_dim=512,
        action_dim=61,
        hidden_dim=256,
        enable_uncertainty=True
    )
    
    model = create_optimized_latent_preference_model(config)
    
    # æµ‹è¯•è¾¹ç•Œæ¡ä»¶
    boundary_tests = [
        (1.0, 0.7, "è¾¹ç•Œé«˜ç½®ä¿¡åº¦æ­£å¥–åŠ±"),
        (-1.0, 0.7, "è¾¹ç•Œé«˜ç½®ä¿¡åº¦è´Ÿå¥–åŠ±"),
        (1.0, 0.4, "è¾¹ç•Œä½ç½®ä¿¡åº¦æ­£å¥–åŠ±"),
        (-1.0, 0.4, "è¾¹ç•Œä½ç½®ä¿¡åº¦è´Ÿå¥–åŠ±"),
    ]
    
    print("æµ‹è¯•è¾¹ç•Œæ¡ä»¶:")
    
    for raw_reward, confidence, description in boundary_tests:
        mapped_reward = model._map_reward_with_confidence(raw_reward, confidence)
        print(f"  {description}: åŸå§‹={raw_reward:5.1f}, ç½®ä¿¡åº¦={confidence:4.1f} -> æ˜ å°„={mapped_reward:6.3f}")
        
        # éªŒè¯è¾¹ç•Œå€¼
        if confidence == 0.7:
            expected = 0.4 if raw_reward > 0 else -0.4
            if abs(mapped_reward - expected) > 1e-6:
                print(f"    âŒ è¾¹ç•Œå€¼ä¸æ­£ç¡®ï¼ŒæœŸæœ› {expected}, å¾—åˆ° {mapped_reward}")
                return False
        elif confidence == 0.4:
            expected = 0.1 if raw_reward > 0 else -0.1
            if abs(mapped_reward - expected) > 1e-6:
                print(f"    âŒ è¾¹ç•Œå€¼ä¸æ­£ç¡®ï¼ŒæœŸæœ› {expected}, å¾—åˆ° {mapped_reward}")
                return False
    
    print("  âœ… è¾¹ç•Œæ¡ä»¶æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•çº¿æ€§æ’å€¼
    print("\næµ‹è¯•çº¿æ€§æ’å€¼:")
    
    confidence_values = [0.3, 0.4, 0.5, 0.6, 0.7]
    
    for conf in confidence_values:
        pos_reward = model._map_reward_with_confidence(1.0, conf)
        neg_reward = model._map_reward_with_confidence(-1.0, conf)
        print(f"  ç½®ä¿¡åº¦ {conf:.1f}: æ­£å¥–åŠ± {pos_reward:.3f}, è´Ÿå¥–åŠ± {neg_reward:.3f}")
    
    # éªŒè¯å•è°ƒæ€§
    pos_rewards = [model._map_reward_with_confidence(1.0, conf) for conf in confidence_values]
    neg_rewards = [model._map_reward_with_confidence(-1.0, conf) for conf in confidence_values]
    
    # æ­£å¥–åŠ±åº”è¯¥éšç½®ä¿¡åº¦å¢åŠ è€Œå¢åŠ 
    if not all(pos_rewards[i] <= pos_rewards[i+1] for i in range(len(pos_rewards)-1)):
        print("  âŒ æ­£å¥–åŠ±ä¸æ»¡è¶³å•è°ƒæ€§")
        return False
    
    # è´Ÿå¥–åŠ±åº”è¯¥éšç½®ä¿¡åº¦å¢åŠ è€Œå‡å°‘ï¼ˆç»å¯¹å€¼å¢åŠ ï¼‰
    if not all(neg_rewards[i] >= neg_rewards[i+1] for i in range(len(neg_rewards)-1)):
        print("  âŒ è´Ÿå¥–åŠ±ä¸æ»¡è¶³å•è°ƒæ€§")
        return False
    
    print("  âœ… çº¿æ€§æ’å€¼å’Œå•è°ƒæ€§æµ‹è¯•é€šè¿‡")
    
    return True

if __name__ == "__main__":
    print("å¼€å§‹ç®€åŒ–çš„åå¥½å¥–åŠ±æ¨¡å‹æµ‹è¯•...")
    
    # è¿è¡Œæµ‹è¯•
    functionality_passed = test_preference_model_functionality()
    batch_passed = test_batch_processing()
    consistency_passed = test_confidence_mapping_consistency()
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    print(f"æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•: {'âœ… é€šè¿‡' if functionality_passed else 'âŒ å¤±è´¥'}")
    print(f"æ‰¹é‡å¤„ç†æµ‹è¯•: {'âœ… é€šè¿‡' if batch_passed else 'âŒ å¤±è´¥'}")
    print(f"ä¸€è‡´æ€§æµ‹è¯•: {'âœ… é€šè¿‡' if consistency_passed else 'âŒ å¤±è´¥'}")
    
    if functionality_passed and batch_passed and consistency_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åå¥½å¥–åŠ±æ¨¡å‹ä¿®æ”¹æˆåŠŸå¹¶ç¬¦åˆç”¨æˆ·è¦æ±‚ã€‚")
        print("\nä¸»è¦åŠŸèƒ½éªŒè¯:")
        print("  âœ… å¥–åŠ±èŒƒå›´æ­£ç¡®æ˜ å°„åˆ° (-0.4, -0.1) å’Œ (0.1, 0.4)")
        print("  âœ… ç½®ä¿¡åº¦ >= 0.7 æ—¶è¾¾åˆ°æœ€å¤§å¥–åŠ±/æƒ©ç½š (Â±0.4)")
        print("  âœ… ç½®ä¿¡åº¦ <= 0.4 æ—¶è¾¾åˆ°æœ€å°å¥–åŠ±/æƒ©ç½š (Â±0.1)")
        print("  âœ… ä¸­é—´ç½®ä¿¡åº¦åŒºé—´ä½¿ç”¨çº¿æ€§æ’å€¼")
        print("  âœ… æ‰¹é‡å¤„ç†åŠŸèƒ½æ­£å¸¸")
        print("  âœ… è¾¹ç•Œæ¡ä»¶å’Œå•è°ƒæ€§æ»¡è¶³è¦æ±‚")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")
    
    print("=" * 60)