#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DPOåå¥½æ–¹æ³•æ¼”ç¤ºè„šæœ¬

æœ¬è„šæœ¬æ¼”ç¤ºQUALITY_BASEDå’ŒHYBRID_DPO_QUALITYæ–¹æ³•çš„å…·ä½“è®¡ç®—è¿‡ç¨‹ï¼Œ
å±•ç¤ºå¦‚ä½•ä¾é DPOå’Œè¿™ä¸¤ç§æ–¹æ³•äº§ç”Ÿåå¥½å¯¹ã€‚
"""

import numpy as np
import torch
import math
from typing import Dict, Tuple, List, Any
from dataclasses import dataclass
from enum import Enum

class LabelType(Enum):
    """åå¥½æ ‡ç­¾ç±»å‹"""
    QUALITY_BASED = "quality_based"
    HYBRID_DPO_QUALITY = "hybrid_dpo_quality"
    DPO_BINARY = "dpo_binary"

@dataclass
class TrajectoryData:
    """è½¨è¿¹æ•°æ®ç»“æ„"""
    obs: np.ndarray
    actions: np.ndarray
    rewards: np.ndarray
    name: str
    
    def get_reward_sum(self) -> float:
        """è·å–å¥–åŠ±æ€»å’Œ"""
        return float(np.sum(self.rewards))
    
    def get_length(self) -> int:
        """è·å–è½¨è¿¹é•¿åº¦"""
        return len(self.rewards)

class QualityEvaluator:
    """è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.survival_weight = 1.0
        self.stability_weight = 0.85
        self.smoothness_weight = 0.90
    
    def calculate_survival_score(self, trajectory_length: int, max_length: int = 100) -> float:
        """è®¡ç®—ç”Ÿå­˜å¾—åˆ†"""
        return min(trajectory_length / max_length, 1.0)
    
    def calculate_stability_score(self, obs: np.ndarray) -> float:
        """è®¡ç®—çŠ¶æ€ç¨³å®šæ€§å¾—åˆ†"""
        if len(obs) < 2:
            return 0.3
        
        # è®¡ç®—çŠ¶æ€å˜åŒ–çš„æ–¹å·®
        state_changes = np.diff(obs, axis=0)
        variance = np.mean(np.var(state_changes, axis=0))
        
        # å°†æ–¹å·®è½¬æ¢ä¸ºç¨³å®šæ€§å¾—åˆ† [0.3, 1.0]
        stability = max(0.3, 1.0 - min(variance / 10.0, 0.7))
        return stability
    
    def calculate_smoothness_score(self, actions: np.ndarray) -> float:
        """è®¡ç®—åŠ¨ä½œå¹³æ»‘æ€§å¾—åˆ†"""
        if len(actions) < 2:
            return 0.3
        
        # è®¡ç®—åŠ¨ä½œå˜åŒ–çš„å¹³å‡ç»å¯¹å·®
        action_changes = np.diff(actions, axis=0)
        smoothness_metric = np.mean(np.abs(action_changes))
        
        # å°†å˜åŒ–è½¬æ¢ä¸ºå¹³æ»‘æ€§å¾—åˆ† [0.3, 1.0]
        smoothness = max(0.3, 1.0 - min(smoothness_metric / 5.0, 0.7))
        return smoothness
    
    def evaluate_trajectory_quality(self, trajectory: TrajectoryData) -> Tuple[float, Dict[str, float]]:
        """è¯„ä¼°è½¨è¿¹è´¨é‡"""
        # è®¡ç®—å„é¡¹å¾—åˆ†
        survival_score = self.calculate_survival_score(trajectory.get_length())
        stability_score = self.calculate_stability_score(trajectory.obs)
        smoothness_score = self.calculate_smoothness_score(trajectory.actions)
        
        # è®¡ç®—åŸºç¡€è´¨é‡å› å­
        base_quality_factor = survival_score * stability_score * smoothness_score
        
        # è®¡ç®—æœ€ç»ˆè´¨é‡åˆ†æ•°
        reward_sum = trajectory.get_reward_sum()
        quality_score = reward_sum * base_quality_factor
        
        feature_scores = {
            'survival_score': survival_score,
            'stability_score': stability_score,
            'smoothness_score': smoothness_score,
            'base_quality_factor': base_quality_factor,
            'reward_sum': reward_sum,
            'quality_score': quality_score
        }
        
        return quality_score, feature_scores

class DPOEvaluator:
    """DPOè¯„ä¼°å™¨"""
    
    def __init__(self, beta: float = 5.0, label_smoothing: float = 0.0):
        self.beta = beta
        self.label_smoothing = label_smoothing
        self.quality_evaluator = QualityEvaluator()
    
    def _heuristic_reward_estimate(self, trajectory: TrajectoryData) -> float:
        """å¯å‘å¼å¥–åŠ±ä¼°è®¡ï¼ˆåŸºäºè´¨é‡è¯„ä¼°å™¨ï¼‰"""
        quality_score, _ = self.quality_evaluator.evaluate_trajectory_quality(trajectory)
        return float(np.clip(quality_score, 0.0, 100.0))  # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    
    def _compute_confidence(self, reward_a: float, reward_b: float, preference_logit: float) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        reward_diff = abs(reward_a - reward_b)
        logit_magnitude = abs(preference_logit)
        
        # åŸºäºå¥–åŠ±å·®å¼‚å’Œlogitå¤§å°è®¡ç®—ç½®ä¿¡åº¦
        confidence = min(0.5 + 0.1 * reward_diff + 0.05 * logit_magnitude, 0.95)
        return confidence
    
    def evaluate_dpo_preference(self, trajectory_a: TrajectoryData, 
                               trajectory_b: TrajectoryData) -> Tuple[float, float]:
        """ä½¿ç”¨DPOæ–¹æ³•è¯„ä¼°è½¨è¿¹åå¥½"""
        # 1. è®¡ç®—è½¨è¿¹å¥–åŠ±ï¼ˆä½¿ç”¨å¯å‘å¼ä¼°è®¡ï¼‰
        reward_a = self._heuristic_reward_estimate(trajectory_a)
        reward_b = self._heuristic_reward_estimate(trajectory_b)
        
        # 2. DPOåå¥½æ¦‚ç‡è®¡ç®—
        reward_diff = float(reward_a) - float(reward_b)
        preference_logit = float(self.beta * reward_diff)
        
        # 3. è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._compute_confidence(reward_a, reward_b, preference_logit)
        
        return preference_logit, confidence

class PreferenceMethodsDemo:
    """åå¥½æ–¹æ³•æ¼”ç¤ºç±»"""
    
    def __init__(self):
        self.quality_evaluator = QualityEvaluator()
        self.dpo_evaluator = DPOEvaluator(beta=5.0)
    
    def _calculate_quality_based_score(self, quality_a: float, quality_b: float) -> Tuple[float, float]:
        """QUALITY_BASEDæ–¹æ³•ï¼šåŸºäºè´¨é‡åˆ†æ•°è®¡ç®—åå¥½åˆ†æ•°å’Œç½®ä¿¡åº¦"""
        quality_diff = quality_a - quality_b
        abs_diff = abs(quality_diff)
        
        print(f"    è´¨é‡å·®å¼‚: {quality_diff:.6f}")
        print(f"    ç»å¯¹å·®å¼‚: {abs_diff:.6f}")
        
        # 1. ä¸ç¡®å®šæ€§åˆ¤æ–­ï¼ˆæä¸¥æ ¼ï¼‰
        uncertainty_range = 0.01  # æå°çš„ä¸ç¡®å®šæ€§é˜ˆå€¼
        min_uncertainty_threshold = 0.1
        
        if abs_diff < uncertainty_range:
            print(f"    -> å·®å¼‚å°äºä¸ç¡®å®šæ€§é˜ˆå€¼({uncertainty_range})ï¼Œæ ‡è®°ä¸ºä¸ç¡®å®š")
            return 0.5, min_uncertainty_threshold  # ä¸ç¡®å®š
        
        # 2. è®¡ç®—åå¥½åˆ†æ•°ï¼ˆé«˜æ•æ„Ÿåº¦ï¼‰
        sigmoid_input = quality_diff * 10.0  # é«˜æ•æ„Ÿåº¦ä¹˜æ•°
        print(f"    Sigmoidè¾“å…¥: {quality_diff} * 10.0 = {sigmoid_input:.3f}")
        
        preference_score = torch.sigmoid(torch.tensor(sigmoid_input)).item()
        print(f"    åŸå§‹åå¥½åˆ†æ•°: {preference_score:.6f}")
        
        # 3. åº”ç”¨æ ‡ç­¾å¹³æ»‘ï¼ˆå‡ ä¹ä¸ä½¿ç”¨ï¼‰
        smoothing = 0.01 * 0.1  # æå°çš„æ ‡ç­¾å¹³æ»‘
        if preference_score > 0.5:
            preference_score = preference_score * (1 - smoothing) + 0.5 * smoothing
        else:
            preference_score = preference_score * (1 - smoothing) + 0.5 * smoothing
        print(f"    æ ‡ç­¾å¹³æ»‘å: {preference_score:.6f} (å¹³æ»‘ç³»æ•°: {smoothing})")
        
        # 4. è®¡ç®—ç½®ä¿¡åº¦ï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰
        confidence = min(abs_diff * 10.0 + 0.5, 0.95)
        print(f"    ç½®ä¿¡åº¦: min({abs_diff:.6f} * 10.0 + 0.5, 0.95) = {confidence:.6f}")
        
        return preference_score, confidence
    
    def _calculate_hybrid_dpo_quality_score(self, trajectory_a: TrajectoryData, 
                                          trajectory_b: TrajectoryData,
                                          quality_a: float, quality_b: float) -> Tuple[float, float]:
        """HYBRID_DPO_QUALITYæ–¹æ³•ï¼šç»“åˆDPOè¯„ä¼°å’Œè´¨é‡è¯„ä¼°"""
        print("    === DPOè¯„ä¼°éƒ¨åˆ† ===")
        # 1. è®¡ç®—DPOåˆ†æ•°
        dpo_logit, dpo_conf = self.dpo_evaluator.evaluate_dpo_preference(trajectory_a, trajectory_b)
        dpo_score = torch.sigmoid(torch.tensor(dpo_logit)).item()
        
        print(f"    DPO logit: {dpo_logit:.6f}")
        print(f"    DPOåˆ†æ•°: sigmoid({dpo_logit:.6f}) = {dpo_score:.6f}")
        print(f"    DPOç½®ä¿¡åº¦: {dpo_conf:.6f}")
        
        print("    === è´¨é‡è¯„ä¼°éƒ¨åˆ† ===")
        # 2. è®¡ç®—è´¨é‡åˆ†æ•°
        quality_score, quality_conf = self._calculate_quality_based_score(quality_a, quality_b)
        
        print(f"    è´¨é‡åˆ†æ•°: {quality_score:.6f}")
        print(f"    è´¨é‡ç½®ä¿¡åº¦: {quality_conf:.6f}")
        
        print("    === æ··åˆè®¡ç®— ===")
        # 3. åŠ æƒç»„åˆï¼ˆDPOä¸»å¯¼ï¼‰
        dpo_weight = 0.8
        quality_weight = 0.2
        
        combined_score = dpo_weight * dpo_score + quality_weight * quality_score
        combined_conf = (dpo_conf + quality_conf) / 2  # å¹³å‡ç½®ä¿¡åº¦
        
        print(f"    ç»„åˆåˆ†æ•°: {dpo_weight} * {dpo_score:.6f} + {quality_weight} * {quality_score:.6f} = {combined_score:.6f}")
        print(f"    ç»„åˆç½®ä¿¡åº¦: ({dpo_conf:.6f} + {quality_conf:.6f}) / 2 = {combined_conf:.6f}")
        
        return combined_score, combined_conf
    
    def create_sample_trajectories(self) -> List[TrajectoryData]:
        """åˆ›å»ºç¤ºä¾‹è½¨è¿¹æ•°æ®"""
        trajectories = []
        
        # è½¨è¿¹Aï¼šé«˜è´¨é‡è½¨è¿¹
        obs_a = np.array([
            [1.0, 0.5, 0.2],
            [1.1, 0.52, 0.21],
            [1.15, 0.54, 0.22],
            [1.2, 0.56, 0.23],
            [1.25, 0.58, 0.24],
            [1.3, 0.6, 0.25],
            [1.35, 0.62, 0.26],
            [1.4, 0.64, 0.27],
            [1.45, 0.66, 0.28],
            [1.5, 0.68, 0.29]
        ])
        actions_a = np.array([
            [0.1, 0.05],
            [0.11, 0.052],
            [0.12, 0.054],
            [0.13, 0.056],
            [0.14, 0.058],
            [0.15, 0.06],
            [0.16, 0.062],
            [0.17, 0.064],
            [0.18, 0.066]
        ])
        rewards_a = np.array([2.5, 2.8, 3.1, 2.9, 3.2, 2.7, 3.0, 2.6, 2.9, 3.1])
        
        trajectory_a = TrajectoryData(obs_a, actions_a, rewards_a, "é«˜è´¨é‡è½¨è¿¹A")
        trajectories.append(trajectory_a)
        
        # è½¨è¿¹Bï¼šä½è´¨é‡è½¨è¿¹
        obs_b = np.array([
            [0.5, 0.2, 0.1],
            [0.3, 0.4, 0.15],
            [0.8, 0.1, 0.05],
            [0.2, 0.6, 0.2],
            [0.9, 0.05, 0.02],
            [0.1, 0.7, 0.25]
        ])
        actions_b = np.array([
            [0.2, 0.1],
            [-0.1, 0.3],
            [0.4, -0.2],
            [-0.3, 0.5],
            [0.6, -0.4]
        ])
        rewards_b = np.array([0.5, 0.2, -0.1, 0.3, -0.2, 0.1])
        
        trajectory_b = TrajectoryData(obs_b, actions_b, rewards_b, "ä½è´¨é‡è½¨è¿¹B")
        trajectories.append(trajectory_b)
        
        # è½¨è¿¹Cï¼šä¸­ç­‰è´¨é‡è½¨è¿¹
        obs_c = np.array([
            [0.8, 0.4, 0.15],
            [0.85, 0.42, 0.16],
            [0.9, 0.44, 0.17],
            [0.95, 0.46, 0.18],
            [1.0, 0.48, 0.19],
            [1.05, 0.5, 0.2],
            [1.1, 0.52, 0.21],
            [1.15, 0.54, 0.22]
        ])
        actions_c = np.array([
            [0.08, 0.04],
            [0.09, 0.042],
            [0.1, 0.044],
            [0.11, 0.046],
            [0.12, 0.048],
            [0.13, 0.05],
            [0.14, 0.052]
        ])
        rewards_c = np.array([1.5, 1.8, 2.0, 1.9, 2.1, 1.7, 2.2, 1.6])
        
        trajectory_c = TrajectoryData(obs_c, actions_c, rewards_c, "ä¸­ç­‰è´¨é‡è½¨è¿¹C")
        trajectories.append(trajectory_c)
        
        return trajectories
    
    def demonstrate_preference_calculation(self):
        """æ¼”ç¤ºåå¥½è®¡ç®—è¿‡ç¨‹"""
        print("ğŸ¯ DPOåå¥½æ–¹æ³•æ¼”ç¤º")
        print("=" * 80)
        
        # åˆ›å»ºç¤ºä¾‹è½¨è¿¹
        trajectories = self.create_sample_trajectories()
        
        # è¯„ä¼°æ¯æ¡è½¨è¿¹çš„è´¨é‡
        print("\nğŸ“Š è½¨è¿¹è´¨é‡è¯„ä¼°")
        print("-" * 50)
        
        trajectory_qualities = []
        for i, traj in enumerate(trajectories):
            quality_score, feature_scores = self.quality_evaluator.evaluate_trajectory_quality(traj)
            trajectory_qualities.append(quality_score)
            
            print(f"\n{traj.name}:")
            print(f"  å¥–åŠ±æ€»å’Œ: {feature_scores['reward_sum']:.3f}")
            print(f"  ç”Ÿå­˜å¾—åˆ†: {feature_scores['survival_score']:.3f}")
            print(f"  ç¨³å®šæ€§å¾—åˆ†: {feature_scores['stability_score']:.3f}")
            print(f"  å¹³æ»‘æ€§å¾—åˆ†: {feature_scores['smoothness_score']:.3f}")
            print(f"  åŸºç¡€è´¨é‡å› å­: {feature_scores['base_quality_factor']:.3f}")
            print(f"  æœ€ç»ˆè´¨é‡åˆ†æ•°: {quality_score:.3f}")
        
        # è¿›è¡Œåå¥½æ¯”è¾ƒ
        comparisons = [
            (0, 1, "é«˜è´¨é‡ vs ä½è´¨é‡"),
            (0, 2, "é«˜è´¨é‡ vs ä¸­ç­‰è´¨é‡"),
            (2, 1, "ä¸­ç­‰è´¨é‡ vs ä½è´¨é‡")
        ]
        
        for idx_a, idx_b, comparison_name in comparisons:
            traj_a = trajectories[idx_a]
            traj_b = trajectories[idx_b]
            quality_a = trajectory_qualities[idx_a]
            quality_b = trajectory_qualities[idx_b]
            
            print(f"\n\nğŸ” åå¥½æ¯”è¾ƒ: {comparison_name}")
            print("=" * 60)
            print(f"è½¨è¿¹A ({traj_a.name}): è´¨é‡åˆ†æ•° = {quality_a:.6f}")
            print(f"è½¨è¿¹B ({traj_b.name}): è´¨é‡åˆ†æ•° = {quality_b:.6f}")
            
            # QUALITY_BASEDæ–¹æ³•
            print("\nğŸ¯ QUALITY_BASEDæ–¹æ³•è®¡ç®—:")
            print("-" * 40)
            quality_pref_score, quality_conf = self._calculate_quality_based_score(quality_a, quality_b)
            
            print(f"\n  ç»“æœ: åå¥½åˆ†æ•° = {quality_pref_score:.6f}, ç½®ä¿¡åº¦ = {quality_conf:.6f}")
            
            if quality_pref_score > 0.7:
                print(f"  âœ… å¼ºåå¥½è½¨è¿¹A (ç½®ä¿¡åº¦: {quality_conf:.1%})")
            elif quality_pref_score > 0.55:
                print(f"  âœ… åå¥½è½¨è¿¹A (ç½®ä¿¡åº¦: {quality_conf:.1%})")
            elif quality_pref_score < 0.3:
                print(f"  âœ… å¼ºåå¥½è½¨è¿¹B (ç½®ä¿¡åº¦: {quality_conf:.1%})")
            elif quality_pref_score < 0.45:
                print(f"  âœ… åå¥½è½¨è¿¹B (ç½®ä¿¡åº¦: {quality_conf:.1%})")
            else:
                print(f"  âš ï¸ åå¥½ä¸æ˜ç¡® (ç½®ä¿¡åº¦: {quality_conf:.1%})")
            
            # HYBRID_DPO_QUALITYæ–¹æ³•
            print("\nğŸ¯ HYBRID_DPO_QUALITYæ–¹æ³•è®¡ç®—:")
            print("-" * 40)
            hybrid_pref_score, hybrid_conf = self._calculate_hybrid_dpo_quality_score(
                traj_a, traj_b, quality_a, quality_b
            )
            
            print(f"\n  ç»“æœ: åå¥½åˆ†æ•° = {hybrid_pref_score:.6f}, ç½®ä¿¡åº¦ = {hybrid_conf:.6f}")
            
            if hybrid_pref_score > 0.7:
                print(f"  âœ… å¼ºåå¥½è½¨è¿¹A (ç½®ä¿¡åº¦: {hybrid_conf:.1%})")
            elif hybrid_pref_score > 0.55:
                print(f"  âœ… åå¥½è½¨è¿¹A (ç½®ä¿¡åº¦: {hybrid_conf:.1%})")
            elif hybrid_pref_score < 0.3:
                print(f"  âœ… å¼ºåå¥½è½¨è¿¹B (ç½®ä¿¡åº¦: {hybrid_conf:.1%})")
            elif hybrid_pref_score < 0.45:
                print(f"  âœ… åå¥½è½¨è¿¹B (ç½®ä¿¡åº¦: {hybrid_conf:.1%})")
            else:
                print(f"  âš ï¸ åå¥½ä¸æ˜ç¡® (ç½®ä¿¡åº¦: {hybrid_conf:.1%})")
            
            # æ–¹æ³•æ¯”è¾ƒ
            print("\nğŸ“ˆ æ–¹æ³•æ¯”è¾ƒ:")
            print("-" * 20)
            print(f"  QUALITY_BASED:     åå¥½åˆ†æ•°={quality_pref_score:.6f}, ç½®ä¿¡åº¦={quality_conf:.6f}")
            print(f"  HYBRID_DPO_QUALITY: åå¥½åˆ†æ•°={hybrid_pref_score:.6f}, ç½®ä¿¡åº¦={hybrid_conf:.6f}")
            
            score_diff = abs(quality_pref_score - hybrid_pref_score)
            conf_diff = abs(quality_conf - hybrid_conf)
            print(f"  åˆ†æ•°å·®å¼‚: {score_diff:.6f}")
            print(f"  ç½®ä¿¡åº¦å·®å¼‚: {conf_diff:.6f}")
    
    def demonstrate_preference_pair_generation(self):
        """æ¼”ç¤ºåå¥½å¯¹ç”Ÿæˆè¿‡ç¨‹"""
        print("\n\nğŸ”„ åå¥½å¯¹ç”Ÿæˆæ¼”ç¤º")
        print("=" * 80)
        
        trajectories = self.create_sample_trajectories()
        
        # è®¡ç®—æ‰€æœ‰è½¨è¿¹çš„è´¨é‡åˆ†æ•°
        scored_trajectories = []
        for traj in trajectories:
            quality_score, _ = self.quality_evaluator.evaluate_trajectory_quality(traj)
            scored_trajectories.append((traj, quality_score))
        
        # æŒ‰è´¨é‡åˆ†æ•°æ’åº
        scored_trajectories.sort(key=lambda x: x[1], reverse=True)
        
        print("\nğŸ“Š è½¨è¿¹è´¨é‡æ’åº:")
        for i, (traj, score) in enumerate(scored_trajectories):
            print(f"  {i+1}. {traj.name}: {score:.6f}")
        
        # ç”Ÿæˆåå¥½å¯¹
        print("\nğŸ¯ ç”Ÿæˆåå¥½å¯¹:")
        preference_pairs = []
        
        # ç­–ç•¥1: é«˜è´¨é‡ vs ä½è´¨é‡ï¼ˆå¼ºå¯¹æ¯”ï¼‰
        for i in range(len(scored_trajectories)):
            for j in range(i+1, len(scored_trajectories)):
                traj_better, score_better = scored_trajectories[i]
                traj_worse, score_worse = scored_trajectories[j]
                
                # è®¡ç®—è´¨é‡å·®å¼‚
                quality_diff = score_better - score_worse
                
                # åªä¿ç•™æœ‰æ„ä¹‰çš„å¯¹æ¯”ï¼ˆå·®å¼‚è¶³å¤Ÿå¤§ï¼‰
                if quality_diff > 1.0:  # é˜ˆå€¼å¯è°ƒ
                    preference_pairs.append({
                        'trajectory_a': traj_better,
                        'trajectory_b': traj_worse,
                        'quality_a': score_better,
                        'quality_b': score_worse,
                        'quality_diff': quality_diff,
                        'expected_preference': 'A',
                        'pair_type': 'strong_contrast'
                    })
        
        print(f"\nç”Ÿæˆäº† {len(preference_pairs)} ä¸ªåå¥½å¯¹:")
        
        for i, pair in enumerate(preference_pairs):
            print(f"\nåå¥½å¯¹ {i+1}:")
            print(f"  è½¨è¿¹A: {pair['trajectory_a'].name} (è´¨é‡: {pair['quality_a']:.6f})")
            print(f"  è½¨è¿¹B: {pair['trajectory_b'].name} (è´¨é‡: {pair['quality_b']:.6f})")
            print(f"  è´¨é‡å·®å¼‚: {pair['quality_diff']:.6f}")
            print(f"  é¢„æœŸåå¥½: {pair['expected_preference']}")
            print(f"  å¯¹æ¯”ç±»å‹: {pair['pair_type']}")
            
            # ä½¿ç”¨ä¸¤ç§æ–¹æ³•è®¡ç®—åå¥½æ ‡ç­¾
            quality_score, quality_conf = self._calculate_quality_based_score(
                pair['quality_a'], pair['quality_b']
            )
            
            hybrid_score, hybrid_conf = self._calculate_hybrid_dpo_quality_score(
                pair['trajectory_a'], pair['trajectory_b'], 
                pair['quality_a'], pair['quality_b']
            )
            
            print(f"  QUALITY_BASEDæ ‡ç­¾: {quality_score:.6f} (ç½®ä¿¡åº¦: {quality_conf:.6f})")
            print(f"  HYBRID_DPO_QUALITYæ ‡ç­¾: {hybrid_score:.6f} (ç½®ä¿¡åº¦: {hybrid_conf:.6f})")

def main():
    """ä¸»å‡½æ•°"""
    demo = PreferenceMethodsDemo()
    
    # æ¼”ç¤ºåå¥½è®¡ç®—
    demo.demonstrate_preference_calculation()
    
    # æ¼”ç¤ºåå¥½å¯¹ç”Ÿæˆ
    demo.demonstrate_preference_pair_generation()
    
    print("\n\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 80)
    print("\næ€»ç»“:")
    print("1. QUALITY_BASEDæ–¹æ³•ç›´æ¥åŸºäºè´¨é‡åˆ†æ•°å·®å¼‚ï¼Œæ•æ„Ÿåº¦é«˜ï¼Œç½®ä¿¡åº¦é«˜")
    print("2. HYBRID_DPO_QUALITYæ–¹æ³•ç»“åˆDPOç†è®ºå’Œè´¨é‡è¯„ä¼°ï¼Œæ›´ç¨³å®š")
    print("3. ä¸¤ç§æ–¹æ³•éƒ½èƒ½æœ‰æ•ˆç”Ÿæˆé«˜è´¨é‡çš„åå¥½å¯¹ç”¨äºè®­ç»ƒ")
    print("4. DPOæ”¹é€ åçš„ç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨åŒ–ç”Ÿæˆå¤§é‡æœ‰å­¦ä¹ ä»·å€¼çš„åå¥½æ•°æ®")

if __name__ == "__main__":
    main()