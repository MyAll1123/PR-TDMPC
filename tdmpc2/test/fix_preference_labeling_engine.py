#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤åå¥½æ ‡ç­¾ç”Ÿæˆå¼•æ“çš„æ ¸å¿ƒé—®é¢˜

åŸºäºé—®é¢˜åˆ†æï¼Œå®æ–½æœ€é«˜ä¼˜å…ˆçº§çš„è§£å†³æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨ç¯å¢ƒå¥–åŠ±æ›¿ä»£å¯å‘å¼ä¼°è®¡
"""

import os
import sys
import shutil
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path("/public/home/yaotianxiao2024/SPE/tdmpc2")
sys.path.append(str(project_root))
sys.path.append(str(project_root / "prm"))

class PreferenceLabelingEngineFixer:
    """åå¥½æ ‡ç­¾ç”Ÿæˆå¼•æ“ä¿®å¤å™¨"""
    
    def __init__(self):
        self.project_root = Path("/public/home/yaotianxiao2024/SPE/tdmpc2")
        self.prm_dir = self.project_root / "prm"
        self.engine_file = self.prm_dir / "preference_labeling_engine.py"
        
    def backup_original_file(self):
        """å¤‡ä»½åŸå§‹æ–‡ä»¶"""
        backup_file = self.engine_file.with_suffix(".py.backup")
        if not backup_file.exists():
            shutil.copy2(self.engine_file, backup_file)
            print(f"âœ… å·²å¤‡ä»½åŸå§‹æ–‡ä»¶åˆ°: {backup_file}")
        else:
            print(f"ğŸ“ å¤‡ä»½æ–‡ä»¶å·²å­˜åœ¨: {backup_file}")
    
    def read_original_file(self):
        """è¯»å–åŸå§‹æ–‡ä»¶å†…å®¹"""
        with open(self.engine_file, 'r', encoding='utf-8') as f:
            return f.read()
    
    def create_fixed_heuristic_method(self):
        """åˆ›å»ºä¿®å¤åçš„å¯å‘å¼æ–¹æ³•"""
        return '''
    def _heuristic_reward_estimate(self, obs_seq: List, act_seq: List, 
                                 trajectory_data: Optional[Dict] = None) -> float:
        """
        ä¿®å¤åçš„å¯å‘å¼å¥–åŠ±ä¼°è®¡ - ç›´æ¥ä½¿ç”¨ç¯å¢ƒå¥–åŠ±
        
        Args:
            obs_seq: è§‚æµ‹åºåˆ—
            act_seq: åŠ¨ä½œåºåˆ—  
            trajectory_data: è½¨è¿¹æ•°æ®ï¼ŒåŒ…å«ç¯å¢ƒå¥–åŠ±ä¿¡æ¯
            
        Returns:
            float: å¥–åŠ±ä¼°è®¡å€¼
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨è½¨è¿¹æ•°æ®ä¸­çš„ç¯å¢ƒå¥–åŠ±
            if trajectory_data and 'env_rewards' in trajectory_data:
                env_rewards = trajectory_data['env_rewards']
                if isinstance(env_rewards, (list, tuple)) and len(env_rewards) > 0:
                    # ä½¿ç”¨ç´¯ç§¯ç¯å¢ƒå¥–åŠ±
                    total_env_reward = sum(env_rewards)
                    # å½’ä¸€åŒ–åˆ°åˆç†èŒƒå›´ [0, 1]
                    normalized_reward = max(0.0, min(1.0, total_env_reward / 100.0))
                    return normalized_reward
            
            # å¦‚æœè½¨è¿¹æ•°æ®ä¸­æœ‰æ€»å¥–åŠ±ä¿¡æ¯
            if trajectory_data and 'total_reward' in trajectory_data:
                total_reward = trajectory_data['total_reward']
                normalized_reward = max(0.0, min(1.0, total_reward / 100.0))
                return normalized_reward
            
            # å¦‚æœè½¨è¿¹æ•°æ®ä¸­æœ‰ç´¯ç§¯å¥–åŠ±ä¿¡æ¯
            if trajectory_data and 'cumulative_reward' in trajectory_data:
                cumulative_reward = trajectory_data['cumulative_reward']
                normalized_reward = max(0.0, min(1.0, cumulative_reward / 100.0))
                return normalized_reward
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ”¹è¿›çš„å¯å‘å¼ä¼°è®¡ï¼ˆä½†æƒé‡è°ƒæ•´ä¸ºæ›´ç¬¦åˆä»»åŠ¡ç›®æ ‡ï¼‰
            return self._improved_heuristic_estimate(obs_seq, act_seq)
            
        except Exception as e:
            self.logger.warning(f"ç¯å¢ƒå¥–åŠ±è·å–å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨å¯å‘å¼ä¼°è®¡: {e}")
            return self._improved_heuristic_estimate(obs_seq, act_seq)
    
    def _improved_heuristic_estimate(self, obs_seq: List, act_seq: List) -> float:
        """
        æ”¹è¿›çš„å¯å‘å¼ä¼°è®¡ - æ›´ç¬¦åˆä»»åŠ¡ç›®æ ‡
        
        Args:
            obs_seq: è§‚æµ‹åºåˆ—
            act_seq: åŠ¨ä½œåºåˆ—
            
        Returns:
            float: æ”¹è¿›çš„å¯å‘å¼å¥–åŠ±ä¼°è®¡
        """
        try:
            # 1. ä»»åŠ¡å®Œæˆåº¦è¯„ä¼°ï¼ˆåŸºäºè½¨è¿¹é•¿åº¦ï¼Œä½†ä¸è¿‡åº¦æƒ©ç½šçŸ­è½¨è¿¹ï¼‰
            survival_score = min(len(obs_seq) / 150.0, 1.0)  # é™ä½é•¿åº¦è¦æ±‚
            
            # 2. é™ä½å¯¹åŠ¨ä½œå¹³æ»‘æ€§çš„è¿‡åº¦è¦æ±‚
            if len(act_seq) > 1:
                act_array = np.array(act_seq)
                action_variance = np.mean(np.var(act_array, axis=0))
                # ä½¿ç”¨æ›´å®½æ¾çš„å¹³æ»‘æ€§è¯„ä¼°
                smoothness_score = np.exp(-action_variance * 0.1)  # é™ä½æƒ©ç½šç³»æ•°
            else:
                smoothness_score = 0.7  # æé«˜é»˜è®¤åˆ†æ•°
            
            # 3. çŠ¶æ€ç¨³å®šæ€§ï¼ˆé™ä½æƒé‡ï¼‰
            if len(obs_seq) > 1:
                obs_array = np.array(obs_seq)
                obs_variance = np.mean(np.var(obs_array, axis=0))
                stability_score = np.exp(-obs_variance * 0.05)  # è¿›ä¸€æ­¥é™ä½æƒ©ç½š
            else:
                stability_score = 0.7  # æé«˜é»˜è®¤åˆ†æ•°
            
            # 4. è°ƒæ•´æƒé‡ï¼šæ›´é‡è§†ä»»åŠ¡å®Œæˆåº¦ï¼Œé™ä½å¯¹"ç¾è§‚åº¦"çš„è¦æ±‚
            heuristic_reward = (
                0.6 * survival_score +      # æé«˜ç”Ÿå­˜/å®Œæˆåº¦æƒé‡
                0.2 * smoothness_score +    # é™ä½å¹³æ»‘æ€§æƒé‡  
                0.2 * stability_score       # é™ä½ç¨³å®šæ€§æƒé‡
            )
            
            # 5. ç¡®ä¿ç»“æœåœ¨åˆç†èŒƒå›´å†…
            return max(0.1, min(0.9, heuristic_reward))
            
        except Exception as e:
            self.logger.warning(f"æ”¹è¿›å¯å‘å¼ä¼°è®¡å¤±è´¥: {e}")
            return 0.5  # è¿”å›ä¸­æ€§å€¼
'''
    
    def create_environment_reward_aware_methods(self):
        """åˆ›å»ºç¯å¢ƒå¥–åŠ±æ„ŸçŸ¥çš„æ–¹æ³•"""
        return '''
    def _extract_env_reward_from_trajectory(self, trajectory_data: Dict) -> Optional[float]:
        """
        ä»è½¨è¿¹æ•°æ®ä¸­æå–ç¯å¢ƒå¥–åŠ±
        
        Args:
            trajectory_data: è½¨è¿¹æ•°æ®å­—å…¸
            
        Returns:
            Optional[float]: ç¯å¢ƒå¥–åŠ±ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
        """
        try:
            # å°è¯•å¤šç§å¯èƒ½çš„ç¯å¢ƒå¥–åŠ±å­—æ®µå
            reward_fields = [
                'env_rewards', 'environment_rewards', 'rewards',
                'total_reward', 'cumulative_reward', 'episode_reward',
                'env_reward', 'reward_sum'
            ]
            
            for field in reward_fields:
                if field in trajectory_data:
                    reward_data = trajectory_data[field]
                    
                    if isinstance(reward_data, (list, tuple)):
                        # å¦‚æœæ˜¯åºåˆ—ï¼Œè®¡ç®—æ€»å’Œ
                        return sum(reward_data)
                    elif isinstance(reward_data, (int, float)):
                        # å¦‚æœæ˜¯å•ä¸ªæ•°å€¼
                        return float(reward_data)
            
            return None
            
        except Exception as e:
            self.logger.warning(f"æå–ç¯å¢ƒå¥–åŠ±å¤±è´¥: {e}")
            return None
    
    def _validate_preference_with_env_reward(self, 
                                           trajectory_a_data: Dict,
                                           trajectory_b_data: Dict,
                                           preference_score: float,
                                           confidence: float) -> Tuple[float, float, bool]:
        """
        ä½¿ç”¨ç¯å¢ƒå¥–åŠ±éªŒè¯åå¥½æ ‡ç­¾çš„ä¸€è‡´æ€§
        
        Args:
            trajectory_a_data: è½¨è¿¹Açš„æ•°æ®
            trajectory_b_data: è½¨è¿¹Bçš„æ•°æ®
            preference_score: åŸå§‹åå¥½åˆ†æ•°
            confidence: åŸå§‹ç½®ä¿¡åº¦
            
        Returns:
            Tuple[float, float, bool]: (è°ƒæ•´åçš„åå¥½åˆ†æ•°, è°ƒæ•´åçš„ç½®ä¿¡åº¦, æ˜¯å¦ä¸€è‡´)
        """
        try:
            # æå–ç¯å¢ƒå¥–åŠ±
            env_reward_a = self._extract_env_reward_from_trajectory(trajectory_a_data)
            env_reward_b = self._extract_env_reward_from_trajectory(trajectory_b_data)
            
            if env_reward_a is None or env_reward_b is None:
                # æ— æ³•è·å–ç¯å¢ƒå¥–åŠ±ï¼Œè¿”å›åŸå§‹å€¼
                return preference_score, confidence, True
            
            # åŸºäºç¯å¢ƒå¥–åŠ±è®¡ç®—æœŸæœ›çš„åå¥½
            env_reward_diff = env_reward_a - env_reward_b
            
            # å¦‚æœç¯å¢ƒå¥–åŠ±å·®å¼‚å¾ˆå°ï¼Œé™ä½ç½®ä¿¡åº¦
            if abs(env_reward_diff) < 5.0:  # é˜ˆå€¼å¯è°ƒ
                confidence = min(confidence, 0.6)
            
            # æ£€æŸ¥åå¥½æ–¹å‘æ˜¯å¦ä¸€è‡´
            env_prefers_a = env_reward_diff > 0
            model_prefers_a = preference_score > 0.5
            
            is_consistent = env_prefers_a == model_prefers_a
            
            if not is_consistent:
                # å¦‚æœä¸ä¸€è‡´ï¼Œæ ¹æ®ç¯å¢ƒå¥–åŠ±è°ƒæ•´åå¥½åˆ†æ•°
                if abs(env_reward_diff) > 10.0:  # ç¯å¢ƒå¥–åŠ±å·®å¼‚è¾ƒå¤§æ—¶
                    # å¼ºåˆ¶ä½¿ç”¨ç¯å¢ƒå¥–åŠ±çš„åå¥½æ–¹å‘
                    adjusted_score = 0.7 if env_prefers_a else 0.3
                    adjusted_confidence = 0.8
                    self.logger.warning(
                        f"åå¥½ä¸ä¸€è‡´å·²ä¿®æ­£: ç¯å¢ƒå¥–åŠ±å·®å¼‚={env_reward_diff:.2f}, "
                        f"åŸå§‹åå¥½={preference_score:.3f} -> è°ƒæ•´å={adjusted_score:.3f}"
                    )
                    return adjusted_score, adjusted_confidence, False
                else:
                    # ç¯å¢ƒå¥–åŠ±å·®å¼‚è¾ƒå°æ—¶ï¼Œé™ä½ç½®ä¿¡åº¦ä½†ä¿æŒåŸåå¥½
                    return preference_score, min(confidence, 0.4), False
            
            return preference_score, confidence, True
            
        except Exception as e:
            self.logger.warning(f"åå¥½éªŒè¯å¤±è´¥: {e}")
            return preference_score, confidence, True
'''
    
    def apply_fixes(self):
        """åº”ç”¨ä¿®å¤"""
        print("ğŸ”§ å¼€å§‹ä¿®å¤åå¥½æ ‡ç­¾ç”Ÿæˆå¼•æ“...")
        
        # 1. å¤‡ä»½åŸå§‹æ–‡ä»¶
        self.backup_original_file()
        
        # 2. è¯»å–åŸå§‹å†…å®¹
        original_content = self.read_original_file()
        
        # 3. æ›¿æ¢å¯å‘å¼å¥–åŠ±ä¼°è®¡æ–¹æ³•
        print("ğŸ“ ä¿®å¤å¯å‘å¼å¥–åŠ±ä¼°è®¡æ–¹æ³•...")
        
        # æŸ¥æ‰¾å¹¶æ›¿æ¢_heuristic_reward_estimateæ–¹æ³•
        import re
        
        # åŒ¹é…åŸå§‹çš„_heuristic_reward_estimateæ–¹æ³•
        pattern = r'(\s+def _heuristic_reward_estimate\([^}]+?\n\s+except[^}]+?return [^\n]+)'
        
        if re.search(pattern, original_content, re.DOTALL):
            # æ›¿æ¢ç°æœ‰æ–¹æ³•
            fixed_content = re.sub(
                pattern,
                self.create_fixed_heuristic_method(),
                original_content,
                flags=re.DOTALL
            )
        else:
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œåœ¨ç±»çš„æœ«å°¾æ·»åŠ æ–°æ–¹æ³•
            # æŸ¥æ‰¾ç±»çš„ç»“æŸä½ç½®
            class_end_pattern = r'(class PreferenceLabelingEngine[^}]+)(\n\n|$)'
            fixed_content = re.sub(
                class_end_pattern,
                r'\1' + self.create_fixed_heuristic_method() + r'\2',
                original_content,
                flags=re.DOTALL
            )
        
        # 4. æ·»åŠ ç¯å¢ƒå¥–åŠ±æ„ŸçŸ¥æ–¹æ³•
        print("ğŸ“ æ·»åŠ ç¯å¢ƒå¥–åŠ±æ„ŸçŸ¥æ–¹æ³•...")
        
        # åœ¨ç±»çš„æœ«å°¾æ·»åŠ æ–°æ–¹æ³•
        class_end_pattern = r'(class PreferenceLabelingEngine[^}]+)(\n\n|$)'
        fixed_content = re.sub(
            class_end_pattern,
            r'\1' + self.create_environment_reward_aware_methods() + r'\2',
            fixed_content,
            flags=re.DOTALL
        )
        
        # 5. å†™å…¥ä¿®å¤åçš„å†…å®¹
        with open(self.engine_file, 'w', encoding='utf-8') as f:
            f.write(fixed_content)
        
        print(f"âœ… ä¿®å¤å®Œæˆï¼å·²æ›´æ–°æ–‡ä»¶: {self.engine_file}")
        
        return True
    
    def create_validation_script(self):
        """åˆ›å»ºéªŒè¯è„šæœ¬"""
        validation_script = self.project_root / "test" / "validate_preference_fix.py"
        
        validation_content = '''
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éªŒè¯åå¥½æ ‡ç­¾ç”Ÿæˆä¿®å¤æ•ˆæœ
"""

import sys
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path("/public/home/yaotianxiao2024/SPE/tdmpc2")
sys.path.append(str(project_root))
sys.path.append(str(project_root / "prm"))

try:
    from preference_labeling_engine import PreferenceLabelingEngine
    print("âœ… æˆåŠŸå¯¼å…¥ä¿®å¤åçš„PreferenceLabelingEngine")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

def test_heuristic_reward_with_env_data():
    """æµ‹è¯•å¸¦ç¯å¢ƒå¥–åŠ±æ•°æ®çš„å¯å‘å¼ä¼°è®¡"""
    print("\nğŸ§ª æµ‹è¯•ç¯å¢ƒå¥–åŠ±æ„ŸçŸ¥çš„å¯å‘å¼ä¼°è®¡...")
    
    engine = PreferenceLabelingEngine()
    
    # æµ‹è¯•ç”¨ä¾‹1ï¼šé«˜ç¯å¢ƒå¥–åŠ±è½¨è¿¹
    obs_seq_1 = [np.random.randn(10) for _ in range(100)]
    act_seq_1 = [np.random.randn(5) for _ in range(100)]
    trajectory_data_1 = {
        'env_rewards': [1.0] * 100,  # æ€»å¥–åŠ±100
        'total_reward': 100.0
    }
    
    # æµ‹è¯•ç”¨ä¾‹2ï¼šä½ç¯å¢ƒå¥–åŠ±è½¨è¿¹
    obs_seq_2 = [np.random.randn(10) for _ in range(50)]
    act_seq_2 = [np.random.randn(5) for _ in range(50)]
    trajectory_data_2 = {
        'env_rewards': [0.1] * 50,  # æ€»å¥–åŠ±5
        'total_reward': 5.0
    }
    
    try:
        reward_1 = engine._heuristic_reward_estimate(obs_seq_1, act_seq_1, trajectory_data_1)
        reward_2 = engine._heuristic_reward_estimate(obs_seq_2, act_seq_2, trajectory_data_2)
        
        print(f"é«˜ç¯å¢ƒå¥–åŠ±è½¨è¿¹ (æ€»å¥–åŠ±100): å¯å‘å¼å¥–åŠ± = {reward_1:.4f}")
        print(f"ä½ç¯å¢ƒå¥–åŠ±è½¨è¿¹ (æ€»å¥–åŠ±5): å¯å‘å¼å¥–åŠ± = {reward_2:.4f}")
        
        if reward_1 > reward_2:
            print("âœ… ä¿®å¤æˆåŠŸï¼šé«˜ç¯å¢ƒå¥–åŠ±è½¨è¿¹è·å¾—æ›´é«˜çš„å¯å‘å¼å¥–åŠ±")
        else:
            print("âŒ ä¿®å¤å¤±è´¥ï¼šå¯å‘å¼å¥–åŠ±ä¸ç¯å¢ƒå¥–åŠ±ä¸ä¸€è‡´")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

def test_preference_validation():
    """æµ‹è¯•åå¥½éªŒè¯åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•åå¥½éªŒè¯åŠŸèƒ½...")
    
    engine = PreferenceLabelingEngine()
    
    # æ¨¡æ‹Ÿè½¨è¿¹æ•°æ®
    trajectory_a = {'env_rewards': [2.0] * 50, 'total_reward': 100.0}
    trajectory_b = {'env_rewards': [0.1] * 50, 'total_reward': 5.0}
    
    # æ¨¡æ‹Ÿé”™è¯¯çš„åå¥½åˆ†æ•°ï¼ˆåå¥½ä½å¥–åŠ±è½¨è¿¹ï¼‰
    wrong_preference = 0.3  # é”™è¯¯åœ°åå¥½B
    confidence = 0.8
    
    try:
        if hasattr(engine, '_validate_preference_with_env_reward'):
            adjusted_score, adjusted_confidence, is_consistent = engine._validate_preference_with_env_reward(
                trajectory_a, trajectory_b, wrong_preference, confidence
            )
            
            print(f"åŸå§‹åå¥½åˆ†æ•°: {wrong_preference:.3f}")
            print(f"è°ƒæ•´ååå¥½åˆ†æ•°: {adjusted_score:.3f}")
            print(f"åŸå§‹ç½®ä¿¡åº¦: {confidence:.3f}")
            print(f"è°ƒæ•´åç½®ä¿¡åº¦: {adjusted_confidence:.3f}")
            print(f"æ˜¯å¦ä¸€è‡´: {is_consistent}")
            
            if not is_consistent and adjusted_score > 0.5:
                print("âœ… åå¥½éªŒè¯æˆåŠŸï¼šé”™è¯¯åå¥½å·²è¢«ä¿®æ­£")
            else:
                print("âŒ åå¥½éªŒè¯å¤±è´¥")
        else:
            print("âš ï¸ åå¥½éªŒè¯æ–¹æ³•æœªæ‰¾åˆ°ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨æ·»åŠ ")
            
    except Exception as e:
        print(f"âŒ åå¥½éªŒè¯æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹éªŒè¯åå¥½æ ‡ç­¾ç”Ÿæˆä¿®å¤æ•ˆæœ...")
    
    test_heuristic_reward_with_env_data()
    test_preference_validation()
    
    print("\nâœ… éªŒè¯å®Œæˆï¼")
'''
        
        with open(validation_script, 'w', encoding='utf-8') as f:
            f.write(validation_content)
        
        print(f"ğŸ“ å·²åˆ›å»ºéªŒè¯è„šæœ¬: {validation_script}")
        return validation_script
    
    def run_complete_fix(self):
        """è¿è¡Œå®Œæ•´ä¿®å¤æµç¨‹"""
        print("ğŸš€ å¼€å§‹åå¥½æ ‡ç­¾ç”Ÿæˆå¼•æ“å®Œæ•´ä¿®å¤æµç¨‹...")
        
        try:
            # 1. åº”ç”¨ä¿®å¤
            success = self.apply_fixes()
            
            if not success:
                print("âŒ ä¿®å¤å¤±è´¥")
                return False
            
            # 2. åˆ›å»ºéªŒè¯è„šæœ¬
            validation_script = self.create_validation_script()
            
            print("\n" + "="*60)
            print("ğŸ“‹ ä¿®å¤æ€»ç»“")
            print("="*60)
            print("\nâœ… å·²å®Œæˆçš„ä¿®å¤:")
            print("  1. ä¿®æ”¹_heuristic_reward_estimateæ–¹æ³•ç›´æ¥ä½¿ç”¨ç¯å¢ƒå¥–åŠ±")
            print("  2. æ·»åŠ æ”¹è¿›çš„å¯å‘å¼ä¼°è®¡ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")
            print("  3. é™ä½å¯¹åŠ¨ä½œå¹³æ»‘æ€§çš„è¿‡åº¦è¦æ±‚")
            print("  4. æé«˜ä»»åŠ¡å®Œæˆåº¦çš„æƒé‡")
            print("  5. æ·»åŠ ç¯å¢ƒå¥–åŠ±æå–å’ŒéªŒè¯æ–¹æ³•")
            
            print("\nğŸ¯ ä¿®å¤æ•ˆæœ:")
            print("  â€¢ åå¥½æ ‡ç­¾å°†ä¼˜å…ˆåŸºäºçœŸå®çš„ç¯å¢ƒå¥–åŠ±ç”Ÿæˆ")
            print("  â€¢ é«˜ç¯å¢ƒå¥–åŠ±çš„è½¨è¿¹å°†è·å¾—æ­£å‘åå¥½")
            print("  â€¢ ä½ç¯å¢ƒå¥–åŠ±çš„è½¨è¿¹å°†è·å¾—è´Ÿå‘åå¥½")
            print("  â€¢ å‡å°‘äº†å¯å‘å¼è§„åˆ™ä¸ä»»åŠ¡ç›®æ ‡çš„çŸ›ç›¾")
            
            print("\nğŸ“ ä¸‹ä¸€æ­¥å»ºè®®:")
            print(f"  1. è¿è¡ŒéªŒè¯è„šæœ¬: python {validation_script}")
            print("  2. æ¸…ç†ç°æœ‰çš„åå¥½æ•°æ®ç¼“å†²åŒº")
            print("  3. é‡æ–°å¯åŠ¨è®­ç»ƒä»¥ä½¿ç”¨ä¿®å¤åçš„åå¥½æ ‡ç­¾")
            print("  4. ç›‘æ§è®­ç»ƒæ—¥å¿—ä¸­çš„åå¥½ç»Ÿè®¡å˜åŒ–")
            
            return True
            
        except Exception as e:
            print(f"âŒ ä¿®å¤è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return False

if __name__ == "__main__":
    fixer = PreferenceLabelingEngineFixer()
    fixer.run_complete_fix()