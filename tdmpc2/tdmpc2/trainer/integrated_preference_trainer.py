#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†æˆåå¥½å­¦ä¹ çš„TD-MPC2è®­ç»ƒå™¨ - å†…å­˜ç¼“å­˜ç‰ˆæœ¬

åŠŸèƒ½ï¼š
1. ç»§æ‰¿è‡ªåŸæœ‰çš„OnlineTrainerï¼Œä¿æŒTD-MPC2çš„æ ¸å¿ƒè®­ç»ƒæµç¨‹
2. é›†æˆHistoricalPreferenceManagerï¼Œå®ç°å†å²æ•°æ®æ”¶é›†å’Œåå¥½æ¨¡å‹è®­ç»ƒ
3. æœ€å°åŒ–å¯¹åŸæœ‰è®­ç»ƒæµç¨‹çš„å½±å“
4. ä½¿ç”¨å†…å­˜ç¼“å­˜ï¼Œé¿å…æ–‡ä»¶IOæ“ä½œ

ç‰¹ç‚¹ï¼š
- æ— ç¼é›†æˆåˆ°ç°æœ‰è®­ç»ƒæµç¨‹
- è‡ªåŠ¨æ”¶é›†å†å²è½¨è¿¹æ•°æ®
- è‡ªåŠ¨åˆ›å»ºå’Œæ›´æ–°åå¥½æ¨¡å‹
- æä¾›åå¥½å¥–åŠ±å¢å¼ºï¼ˆå¯é€‰ï¼‰
- é«˜æ€§èƒ½å†…å­˜ç¼“å­˜
"""

import os
import sys
import time
import torch
import numpy as np
from typing import Dict, List, Optional, Any
from collections import defaultdict, deque
from termcolor import colored
from tensordict.tensordict import TensorDict

# å¯¼å…¥TD-MPC2æ ¸å¿ƒæ¨¡å—
try:
    from .online_trainer import OnlineTrainer
    from .historical_preference_manager import HistoricalPreferenceManager
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    from online_trainer import OnlineTrainer
    from historical_preference_manager import HistoricalPreferenceManager

# å°è¯•å¯¼å…¥ä¼˜åŒ–åçš„åå¥½ç³»ç»Ÿ
try:
    import sys
    import os
    # æ·»åŠ SPEé¡¹ç›®æ ¹è·¯å¾„
    spe_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "..", ".."))
    if spe_root not in sys.path:
        sys.path.insert(0, spe_root)
    
    from prm.optimized_preference_integrator import OptimizedPreferenceIntegrator
    from prm.optimized_preference_trainer import OptimizedPreferenceTrainer
    from prm.optimized_models.optimized_preference_wrapper import create_optimized_preference_system
    OPTIMIZED_PREFERENCE_AVAILABLE = True
    print("[IntegratedPreferenceTrainer] âœ… ä¼˜åŒ–åå¥½ç³»ç»Ÿå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    OPTIMIZED_PREFERENCE_AVAILABLE = False
    print(f"[IntegratedPreferenceTrainer] âš ï¸ ä¼˜åŒ–åå¥½ç³»ç»Ÿå¯¼å…¥å¤±è´¥: {e}")
    print("[IntegratedPreferenceTrainer] å°†ä½¿ç”¨åŸºç¡€åå¥½å­¦ä¹ åŠŸèƒ½")

class IntegratedPreferenceTrainer(OnlineTrainer):
    """é›†æˆåå¥½å­¦ä¹ çš„TD-MPC2è®­ç»ƒå™¨ - å†…å­˜ç¼“å­˜ç‰ˆæœ¬"""
    
    def __init__(self, *args, **kwargs):
        """åˆå§‹åŒ–é›†æˆåå¥½å­¦ä¹ è®­ç»ƒå™¨"""
        
        # å…ˆè°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(*args, **kwargs)
        
        # ä¼˜å…ˆçº§åå¥½é›†æˆå™¨ï¼ˆå¤–éƒ¨æ³¨å…¥ï¼‰
        self.prioritized_integrator = None
        self.current_episode_id = None  # å½“å‰episodeçš„ID
        
        # è®­ç»ƒé¢‘ç‡æ§åˆ¶
        self.episodes_completed = 0  # å·²å®Œæˆçš„episodeæ•°é‡
        self.last_preference_training_episode = 0  # ä¸Šæ¬¡åå¥½æ¨¡å‹è®­ç»ƒçš„episode
        
        # åˆå§‹åŒ–åå¥½å­¦ä¹ ç›¸å…³ç»„ä»¶
        self._init_historical_preference_manager()
        
        # åˆå§‹åŒ–ä¼˜åŒ–åå¥½ç³»ç»Ÿ
        self._init_optimized_preference_system()
        
        # å½“å‰episodeçš„æ•°æ®ç¼“å†²ï¼ˆå†…å­˜ï¼‰
        self.current_episode_obs = []
        self.current_episode_actions = []
        self.current_episode_rewards = []
        self.current_episode_latent_states = []  # æ–°å¢ï¼šæ½œç©ºé—´çŠ¶æ€ç¼“å†²
        
        # ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ·»åŠ æ½œç©ºé—´åå¥½å¥–åŠ±ç»Ÿè®¡ï¼‰
        self.preference_stats = {
            'historical_data_collections': 0,
            'preference_model_updates': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            # åŒè·¯å¾„å¥–åŠ±èåˆç»Ÿè®¡
            'latent_preference_computations': 0,
            'reward_fusions': 0,
            'total_preference_reward': 0.0,
            'total_environment_reward': 0.0,
            'total_integrated_reward': 0.0,
        }
        
        # æ€§èƒ½ç›‘æ§ï¼ˆæ·»åŠ æ½œç©ºé—´åå¥½è®¡ç®—æ—¶é—´ç›‘æ§ï¼‰
        self.performance_monitor = {
            'data_collection_time': deque(maxlen=100),
            'model_update_time': deque(maxlen=20),
            'model_training_time': deque(maxlen=20),
            # åŒè·¯å¾„å¥–åŠ±èåˆæ€§èƒ½ç›‘æ§
            'reward_fusion_time': deque(maxlen=100),
            'latent_preference_computation_time': deque(maxlen=100),
        }
        
        print(f"[IntegratedPreferenceTrainer] åˆå§‹åŒ–å®Œæˆ - å†…å­˜ç¼“å­˜æ¨¡å¼")
        if self.historical_preference_manager:
            print(f"  - å†å²åå¥½ç®¡ç†å™¨: å·²å¯ç”¨")
            print(f"  - å†…å­˜ç¼“å­˜æ¨¡å¼: å¯ç”¨")
            print(f"  - æ–‡ä»¶IOæ“ä½œ: å·²ç¦ç”¨")
        else:
            print(f"  - å†å²åå¥½ç®¡ç†å™¨: æœªå¯ç”¨")
        
        if hasattr(self, 'latent_preference_integrator') and self.latent_preference_integrator:
            print(f"  - æ½œç©ºé—´åå¥½ç³»ç»Ÿ: å·²å¯ç”¨")
            print(f"  - åŒè·¯å¾„å¥–åŠ±èåˆ: å¯ç”¨")
        else:
            print(f"  - æ½œç©ºé—´åå¥½ç³»ç»Ÿ: æœªå¯ç”¨")
            
        if self.prioritized_integrator:
            print(f"  - ä¼˜å…ˆçº§åå¥½ç³»ç»Ÿ: å·²å¯ç”¨")
        else:
            print(f"  - ä¼˜å…ˆçº§åå¥½ç³»ç»Ÿ: æœªå¯ç”¨")
    
    def set_prioritized_integrator(self, integrator):
        """è®¾ç½®ä¼˜å…ˆçº§åå¥½é›†æˆå™¨"""
        self.prioritized_integrator = integrator
        print(f"[IntegratedPreferenceTrainer] ä¼˜å…ˆçº§åå¥½é›†æˆå™¨å·²è®¾ç½®")
        
        # å¦‚æœå·²ç»æœ‰åå¥½è®­ç»ƒå™¨ï¼Œç«‹å³å°†åå¥½æ¨¡å‹ä¼ é€’ç»™ä¼˜å…ˆçº§ç³»ç»Ÿ
        if self.preference_trainer and hasattr(self.preference_trainer, 'models') and len(self.preference_trainer.models) > 0:
            preference_model = self.preference_trainer.models[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹
            print(f"[IntegratedPreferenceTrainer] æ­£åœ¨å°†åå¥½æ¨¡å‹ä¼ é€’ç»™ä¼˜å…ˆçº§ç³»ç»Ÿ: {type(preference_model).__name__}")
            
            # ç›´æ¥è®¾ç½®åˆ°ä¼˜å…ˆçº§ç³»ç»Ÿä¸­
            if hasattr(integrator, 'prioritized_system') and integrator.prioritized_system:
                integrator.prioritized_system.preference_model = preference_model
                print(f"[IntegratedPreferenceTrainer] âœ… åå¥½æ¨¡å‹å·²æˆåŠŸä¼ é€’ç»™ä¼˜å…ˆçº§ç³»ç»Ÿ")
            else:
                print(f"[IntegratedPreferenceTrainer] âš ï¸ ä¼˜å…ˆçº§é›†æˆå™¨æ²¡æœ‰prioritized_systemå±æ€§")
        else:
            print(f"[IntegratedPreferenceTrainer] âš ï¸ åå¥½è®­ç»ƒå™¨æˆ–æ¨¡å‹å°šæœªå°±ç»ªï¼Œç¨åä¼ é€’åå¥½æ¨¡å‹")
    
    def _init_historical_preference_manager(self):
        """åˆå§‹åŒ–å†å²åå¥½ç®¡ç†å™¨ - å·²ç¦ç”¨åŸå§‹åå¥½å­¦ä¹ æµç¨‹"""
        # åŸå§‹åå¥½å­¦ä¹ æµç¨‹å·²ç¦ç”¨ï¼Œä¸å†åˆå§‹åŒ–å†å²åå¥½ç®¡ç†å™¨
        self.historical_preference_manager = None
        print(f"[IntegratedPreferenceTrainer] âš ï¸ å†å²åå¥½ç®¡ç†å™¨å·²ç¦ç”¨ - ä½¿ç”¨ä¼˜å…ˆçº§åå¥½ç³»ç»Ÿ")
    
    def _init_optimized_preference_system(self):
        """åˆå§‹åŒ–ä¼˜åŒ–åå¥½ç³»ç»Ÿ"""
        self.preference_integrator = None
        self.preference_trainer = None
        
        if not OPTIMIZED_PREFERENCE_AVAILABLE:
            print(f"[IntegratedPreferenceTrainer] ä¼˜åŒ–åå¥½ç³»ç»Ÿä¸å¯ç”¨ï¼Œè·³è¿‡åˆå§‹åŒ–")
            return
        
        try:
            # è·å–ä»»åŠ¡é…ç½®ä¿¡æ¯
            task_name = getattr(self.cfg, 'task', 'unknown')
            
            # æ ¹æ®ä»»åŠ¡ç¡®å®šåŠ¨ä½œç»´åº¦
            action_dim = 61  # humanoid_h1hand é»˜è®¤åŠ¨ä½œç»´åº¦
            if hasattr(self.env, 'action_space'):
                if hasattr(self.env.action_space, 'shape'):
                    action_dim = self.env.action_space.shape[0]
                elif hasattr(self.env.action_space, 'n'):
                    action_dim = self.env.action_space.n
            
            # è·å–æ½œç©ºé—´ç»´åº¦
            latent_dim = getattr(self.cfg, 'latent_dim', 512)
            
            print(f"[IntegratedPreferenceTrainer] åˆå§‹åŒ–ä¼˜åŒ–åå¥½ç³»ç»Ÿ...")
            print(f"  - ä»»åŠ¡: {task_name}")
            print(f"  - æ½œç©ºé—´ç»´åº¦: {latent_dim}")
            print(f"  - åŠ¨ä½œç»´åº¦: {action_dim}")
            
            # åˆ›å»ºä¼˜åŒ–åå¥½ç³»ç»Ÿï¼Œä¼ é€’TD-MPC2é…ç½®
            self.preference_trainer, self.preference_integrator = create_optimized_preference_system(
                tdmpc2_cfg=self.cfg
            )
            
            # ä¸ºå…¼å®¹æ€§åˆ›å»ºåˆ«å
            self.latent_preference_integrator = self.preference_integrator
            
            # å¦‚æœæ™ºèƒ½ä½“æœ‰åå¥½ç³»ç»Ÿï¼Œæ›´æ–°å®ƒ
            if hasattr(self.agent, 'preference_integrator') and hasattr(self.agent, 'preference_trainer'):
                self.agent.preference_integrator = self.preference_integrator
                self.agent.preference_trainer = self.preference_trainer
                print(f"[IntegratedPreferenceTrainer] âœ… æ™ºèƒ½ä½“åå¥½ç³»ç»Ÿå·²æ›´æ–°")
            
            # å¦‚æœä¼˜å…ˆçº§é›†æˆå™¨å·²ç»å­˜åœ¨ï¼Œå°†åå¥½æ¨¡å‹ä¼ é€’ç»™å®ƒ
            if self.prioritized_integrator and self.preference_trainer and hasattr(self.preference_trainer, 'models') and len(self.preference_trainer.models) > 0:
                preference_model = self.preference_trainer.models[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹
                print(f"[IntegratedPreferenceTrainer] æ­£åœ¨å°†åå¥½æ¨¡å‹ä¼ é€’ç»™å·²å­˜åœ¨çš„ä¼˜å…ˆçº§ç³»ç»Ÿ: {type(preference_model).__name__}")
                
                # ç›´æ¥è®¾ç½®åˆ°ä¼˜å…ˆçº§ç³»ç»Ÿä¸­
                if hasattr(self.prioritized_integrator, 'prioritized_system') and self.prioritized_integrator.prioritized_system:
                    self.prioritized_integrator.prioritized_system.preference_model = preference_model
                    print(f"[IntegratedPreferenceTrainer] âœ… åå¥½æ¨¡å‹å·²æˆåŠŸä¼ é€’ç»™å·²å­˜åœ¨çš„ä¼˜å…ˆçº§ç³»ç»Ÿ")
                else:
                    print(f"[IntegratedPreferenceTrainer] âš ï¸ ä¼˜å…ˆçº§é›†æˆå™¨æ²¡æœ‰prioritized_systemå±æ€§")
            else:
                print(f"[IntegratedPreferenceTrainer] â„¹ï¸ ä¼˜å…ˆçº§é›†æˆå™¨å°šæœªè®¾ç½®æˆ–åå¥½æ¨¡å‹å°šæœªå°±ç»ª")
            
            print(f"[IntegratedPreferenceTrainer] âœ… ä¼˜åŒ–åå¥½ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            print(f"  - åå¥½é›†æˆå™¨: {type(self.preference_integrator).__name__}")
            print(f"  - åå¥½è®­ç»ƒå™¨: {type(self.preference_trainer).__name__}")
            print(f"  - æ™ºèƒ½ä½“é›†æˆ: {'æ˜¯' if hasattr(self.agent, 'preference_integrator') else 'å¦'}")
            
        except Exception as e:
            print(f"[ERROR] ä¼˜åŒ–åå¥½ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.preference_integrator = None
            self.preference_trainer = None
            self.latent_preference_integrator = None
    
    def _collect_step_data(self, obs, action, reward, done=False):
        """æ”¶é›†å•æ­¥æ•°æ®åˆ°åå¥½å­¦ä¹ ç³»ç»Ÿ"""
        if self.historical_preference_manager is None and self.preference_integrator is None and self.prioritized_integrator is None:
            return
        
        start_time = time.time()
        
        try:
            # æ·»åŠ åˆ°å½“å‰episodeç¼“å†²
            self.current_episode_obs.append(obs.copy() if isinstance(obs, np.ndarray) else np.array(obs))
            self.current_episode_actions.append(action.copy() if isinstance(action, np.ndarray) else np.array(action))
            self.current_episode_rewards.append(reward)
            
            # å¦‚æœæœ‰ä¼˜å…ˆçº§é›†æˆå™¨ï¼Œæ”¶é›†æ­¥éª¤æ•°æ®
            if self.prioritized_integrator and self.current_episode_id is not None:
                try:
                    # ç¡®ä¿obså’Œactionæ˜¯numpyæ•°ç»„
                    obs_np = obs.copy() if isinstance(obs, np.ndarray) else np.array(obs.cpu().numpy() if hasattr(obs, 'cpu') else obs)
                    action_np = action.copy() if isinstance(action, np.ndarray) else np.array(action.cpu().numpy() if hasattr(action, 'cpu') else action)
                    
                    self.prioritized_integrator.collect_step(
                        self.current_episode_id, obs_np, action_np, reward, done
                    )
                except Exception as e:
                    print(f"[WARNING] ä¼˜å…ˆçº§é›†æˆå™¨collect_stepå¤±è´¥: {e}")
            
            # å¦‚æœæœ‰æ½œç©ºé—´åå¥½ç³»ç»Ÿï¼Œæ”¶é›†æ½œç©ºé—´çŠ¶æ€
            if self.latent_preference_integrator and hasattr(self.agent, 'model') and hasattr(self.agent.model, 'encode'):
                try:
                    # å°†è§‚æµ‹è½¬æ¢ä¸ºæ½œç©ºé—´çŠ¶æ€
                    with torch.no_grad():
                        if isinstance(obs, np.ndarray):
                            obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)
                        else:
                            obs_tensor = obs.float().unsqueeze(0) if obs.dim() == 1 else obs.float()
                        
                        # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                        obs_tensor = obs_tensor.to(self.agent.device)
                        
                        # ä½¿ç”¨TD-MPC2çš„encodeæ–¹æ³•è·å–æ½œç©ºé—´è¡¨ç¤º
                        # å¯¹äºå•ä»»åŠ¡åœºæ™¯ï¼Œtaskå‚æ•°ä¸ºNone
                        task = None
                        if hasattr(self.cfg, 'multitask') and self.cfg.multitask:
                            # å¤šä»»åŠ¡åœºæ™¯ä¸‹éœ€è¦æä¾›taskå‚æ•°
                            task = torch.tensor([0], device=self.agent.device)  # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡
                        
                        latent_state = self.agent.model.encode(obs_tensor, task)
                        if latent_state.dim() > 1:
                            latent_state = latent_state.squeeze(0)  # ç§»é™¤æ‰¹æ¬¡ç»´åº¦
                        
                        self.current_episode_latent_states.append(latent_state.cpu())
                        
                except Exception as e:
                    # æ½œç©ºé—´è½¬æ¢å¤±è´¥ä¸å½±å“è®­ç»ƒ
                    if self._step % 1000 == 0:
                        print(f"[WARNING] æ½œç©ºé—´çŠ¶æ€è½¬æ¢å¤±è´¥: {e}")
            
            # åŸå§‹åå¥½å­¦ä¹ æµç¨‹å·²ç¦ç”¨ï¼Œä¸å†å‘å†å²åå¥½ç®¡ç†å™¨æ·»åŠ æ•°æ®
            # if self.historical_preference_manager:
            #     self.historical_preference_manager.add_step_data(...)
            
            # è®°å½•æ€§èƒ½
            collection_time = time.time() - start_time
            self.performance_monitor['data_collection_time'].append(collection_time)
            
        except Exception as e:
            print(f"[WARNING] æ”¶é›†æ­¥éª¤æ•°æ®å¤±è´¥: {e}")
    
    def _finalize_episode_data(self):
        """å®Œæˆepisodeæ•°æ®æ”¶é›†"""
        # å¦‚æœæœ‰ä¼˜å…ˆçº§é›†æˆå™¨ï¼Œè°ƒç”¨å…¶end_episodeæ–¹æ³•
        if self.prioritized_integrator and self.current_episode_id is not None:
            try:
                self.prioritized_integrator.end_episode(self.current_episode_id)
            except Exception as e:
                pass  # é™é»˜å¤„ç†å¤±è´¥
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.current_episode_obs.clear()
        self.current_episode_actions.clear()
        self.current_episode_rewards.clear()
        self.current_episode_latent_states.clear()
        
        # é‡ç½®episode_id
        self.current_episode_id = None
    
    def _check_and_create_preference_model(self):
        """æ£€æŸ¥å¹¶åˆ›å»ºåå¥½æ¨¡å‹ - å·²ç¦ç”¨åŸå§‹åå¥½å­¦ä¹ æµç¨‹"""
        # åŸå§‹åå¥½å­¦ä¹ æµç¨‹å·²ç¦ç”¨ï¼Œåªä½¿ç”¨ä¼˜å…ˆçº§åå¥½ç³»ç»Ÿ
        pass
    
    def _update_agent_preference_model(self):
        """æ›´æ–°æ™ºèƒ½ä½“çš„åå¥½æ¨¡å‹"""
        if self.historical_preference_manager is None:
            return
        
        try:
            # è·å–æœ€æ–°çš„åå¥½æ¨¡å‹
            preference_model = self.historical_preference_manager.get_preference_model()
            
            if preference_model is not None and hasattr(self.agent, 'update_preference_model'):
                print(f"[IntegratedPreferenceTrainer] æ›´æ–°æ™ºèƒ½ä½“åå¥½æ¨¡å‹...")
                self.agent.update_preference_model(preference_model)
                
                # å¯ç”¨åå¥½æ„ŸçŸ¥è§„åˆ’
                if hasattr(self.agent, 'enable_preference_planning'):
                    self.agent.enable_preference_planning = True
                    print(f"[IntegratedPreferenceTrainer] âœ… åå¥½æ„ŸçŸ¥è§„åˆ’å·²å¯ç”¨")
                
                print(f"[IntegratedPreferenceTrainer] âœ… æ™ºèƒ½ä½“åå¥½æ¨¡å‹æ›´æ–°å®Œæˆ")
            else:
                print(f"[IntegratedPreferenceTrainer] âš ï¸ æ— æ³•æ›´æ–°æ™ºèƒ½ä½“åå¥½æ¨¡å‹ (æ¨¡å‹ä¸ºç©ºæˆ–æ™ºèƒ½ä½“ä¸æ”¯æŒ)")
                
        except Exception as e:
            print(f"[ERROR] æ›´æ–°æ™ºèƒ½ä½“åå¥½æ¨¡å‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # _get_preference_rewardæ–¹æ³•å·²ç§»é™¤ï¼ˆæ¢å¤åŸå§‹TD-MPC2æµç¨‹ï¼‰
    
    def _log_preference_stats(self):
        """è®°å½•åå¥½å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯"""
        if self.historical_preference_manager is None:
            return
        
        try:
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            manager_stats = self.historical_preference_manager.get_stats()
            
            # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
            combined_stats = {
                **self.preference_stats,
                **manager_stats,
                'performance': {
                    'avg_data_collection_time': np.mean(self.performance_monitor['data_collection_time']) if self.performance_monitor['data_collection_time'] else 0,
                    'avg_model_training_time': np.mean(self.performance_monitor['model_training_time']) if self.performance_monitor['model_training_time'] else 0,
                    'avg_reward_fusion_time': np.mean(self.performance_monitor['reward_fusion_time']) if self.performance_monitor.get('reward_fusion_time') else 0,
                    'avg_latent_preference_computation_time': np.mean(self.performance_monitor['latent_preference_computation_time']) if self.performance_monitor.get('latent_preference_computation_time') else 0,
                }
            }
            
            # è®°å½•åˆ°æ—¥å¿—
            if hasattr(self, 'logger'):
                # ä½¿ç”¨ train category ä»£æ›¿ preference_stats
                self.logger.log(combined_stats, "train")
            
            # ç²¾ç®€ç»Ÿè®¡ä¿¡æ¯æ‰“å°ï¼ˆæ¯500ä¸ªepisodeæ‰“å°ä¸€æ¬¡ï¼‰
            if self._ep_idx % 500 == 0:
                print(f"\n=== åå¥½å­¦ä¹ ç»Ÿè®¡ (Episode {self._ep_idx}) ===")
                print(f"æ•°æ®æ”¶é›†: {self.preference_stats['historical_data_collections']}, æ¨¡å‹æ›´æ–°: {self.preference_stats['preference_model_updates']}, ç¼“å­˜è½¨è¿¹: {manager_stats.get('total_trajectories', 0)}")
                if self.latent_preference_integrator:
                    print(f"å¥–åŠ±èåˆ: {self.preference_stats.get('reward_fusions', 0)} æ¬¡")
                    
                    if self.preference_stats.get('reward_fusions', 0) > 0:
                        # è®¡ç®—å¹³å‡å¥–åŠ±
                        avg_pref_reward = self.preference_stats['total_preference_reward'] / self.preference_stats['reward_fusions']
                        avg_env_reward = self.preference_stats['total_environment_reward'] / self.preference_stats['reward_fusions']
                        avg_integrated_reward = self.preference_stats['total_integrated_reward'] / self.preference_stats['reward_fusions']
                        
                        print(f"  ğŸ“Š å¹³å‡åå¥½å¥–åŠ±: {avg_pref_reward:.4f}")
                        print(f"  ğŸ“Š å¹³å‡ç¯å¢ƒå¥–åŠ±: {avg_env_reward:.4f}")
                        print(f"  ğŸ“Š å¹³å‡é›†æˆå¥–åŠ±: {avg_integrated_reward:.4f}")
                        print(f"  ğŸ“ˆ å¥–åŠ±æå‡: {avg_integrated_reward - avg_env_reward:+.4f}")
                        
                        # èåˆæ•ˆæœåˆ†æ
                        improvement_ratio = (avg_integrated_reward - avg_env_reward) / abs(avg_env_reward) * 100 if avg_env_reward != 0 else 0
                        if improvement_ratio > 1:
                            print(f"  âœ… åå¥½ç³»ç»Ÿæ˜¾è‘—æå‡æ€§èƒ½ (+{improvement_ratio:.2f}%)")
                        elif improvement_ratio > 0:
                            print(f"  âœ… åå¥½ç³»ç»Ÿè½»å¾®æå‡æ€§èƒ½ (+{improvement_ratio:.2f}%)")
                        elif improvement_ratio < -1:
                            print(f"  âš ï¸ åå¥½ç³»ç»Ÿæ˜¾è‘—é™ä½æ€§èƒ½ ({improvement_ratio:.2f}%)")
                        else:
                            print(f"  â– åå¥½ç³»ç»Ÿå½±å“å¾®å¼± ({improvement_ratio:.2f}%)")
                    else:
                        print(f"  âš ï¸ å°šæœªè¿›è¡Œå¥–åŠ±èåˆ")
                    
                    # å¥–åŠ±èåˆæ€§èƒ½ç»Ÿè®¡
                    if self.performance_monitor.get('reward_fusion_time'):
                        avg_fusion_time = np.mean(self.performance_monitor['reward_fusion_time'])
                        print(f"  â±ï¸ å¹³å‡å¥–åŠ±èåˆæ—¶é—´: {avg_fusion_time*1000:.2f}ms")
                        print(f"  ğŸ“Š èåˆé¢‘ç‡: {len(self.performance_monitor['reward_fusion_time'])/100:.2f} æ¬¡/episode")
                
                print("=" * 60)
                
        except Exception as e:
            print(f"[WARNING] è®°å½•åå¥½å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
    
    def _cleanup_cache_if_needed(self):
        """æ ¹æ®éœ€è¦æ¸…ç†ç¼“å­˜"""
        if self.historical_preference_manager is None:
            return
        
        try:
            # æ¯1000ä¸ªepisodeæ¸…ç†ä¸€æ¬¡ç¼“å­˜
            if self._ep_idx % 1000 == 0 and self._ep_idx > 0:
                print(f"[IntegratedPreferenceTrainer] å®šæœŸæ¸…ç†ç¼“å­˜ (Episode {self._ep_idx})")
                self.historical_preference_manager.cleanup_cache()
                
        except Exception as e:
            print(f"[WARNING] æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯ï¼ˆé‡å†™çˆ¶ç±»æ–¹æ³•ä»¥é›†æˆåå¥½å­¦ä¹ ï¼‰"""
        print(f"[IntegratedPreferenceTrainer] å¼€å§‹è®­ç»ƒ - é›†æˆåå¥½å­¦ä¹ æ¨¡å¼")
        print(f"  - åå¥½å­¦ä¹ ç³»ç»Ÿ: {'å¯ç”¨' if self.historical_preference_manager else 'ç¦ç”¨'}")
        
        # ä½¿ç”¨çˆ¶ç±»çš„è®­ç»ƒé€»è¾‘ï¼Œä½†åœ¨å…³é”®ç‚¹æ’å…¥åå¥½å­¦ä¹ åŠŸèƒ½
        train_metrics, done, eval_next = {}, True, True
        
        while self._step <= self.cfg.steps:
            # è¯„ä¼°
            if self._step % self.cfg.eval_freq == 0:
                eval_next = True

            # é‡ç½®ç¯å¢ƒ
            if done:
                if eval_next:
                    eval_metrics = self.eval()  # ä¸ä¿å­˜è§†é¢‘
                    eval_metrics.update(self.common_metrics())
                    self.logger.log(eval_metrics, "eval")
                    eval_next = False

                if self._step > 0:
                    # è®¡ç®—episodeå¥–åŠ±ï¼ˆå¦‚æœç¯å¢ƒæ²¡æœ‰è·Ÿè¸ªepisodeå¥–åŠ±ï¼‰
                    episode_env_reward = getattr(self, '_episode_env_reward', 0.0)
                    
                    if episode_env_reward == 0.0:
                        episode_env_reward = torch.tensor(
                            [td["reward"] for td in self._tds[1:]]
                        ).sum().item()
                    
                    # è®¡ç®—é›†æˆå¥–åŠ±ç»Ÿè®¡
                    episode_integrated_reward = getattr(self, '_episode_integrated_reward', episode_env_reward)
                    
                    # === Episodeç»“æŸæ—¶çš„åŒè·¯å¾„å¥–åŠ±èåˆç»Ÿè®¡ ===
                    if self.latent_preference_integrator and hasattr(self, '_episode_integrated_reward'):
                        reward_difference = episode_integrated_reward - episode_env_reward
                        
                        # è¾“å‡ºepisodeèåˆæ±‡æ€»ï¼ˆæ¯ä¸ªepisodeç»“æŸæ—¶è¾“å‡ºä¸€æ¬¡ï¼‰
                        if hasattr(self, '_episode_fusion_data') and self._episode_fusion_data['fusion_count'] > 0:
                            data = self._episode_fusion_data
                            count = data['fusion_count']
                            
                            # è®¡ç®—å¹³å‡å€¼
                            avg_env_reward = data['total_env_reward'] / count
                            avg_pref_reward = data['total_pref_reward'] / count
                            avg_integrated_reward = data['total_integrated_reward'] / count
                            avg_fusion_time = data['total_fusion_time'] / count
                            avg_confidence = data['avg_confidence'] / count
                            avg_pref_weight = data['avg_pref_weight'] / count
                            avg_env_weight = data['avg_env_weight'] / count
                            print(f"  ğŸŒ å¹³å‡ç¯å¢ƒå¥–åŠ±: {avg_env_reward:.4f}")
                            print(f"  ğŸ§  å¹³å‡åå¥½å¥–åŠ±: {avg_pref_reward:.4f}")
                            print(f"  ğŸ¯ å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.4f}")
                            print(f"  â±ï¸ å¹³å‡èåˆè€—æ—¶: {avg_fusion_time*1000:.2f}ms")
                            print(f"  âœ… æ­£å‘åå¥½: {data['positive_preference_count']} | âš ï¸ è´Ÿå‘åå¥½: {data['negative_preference_count']}")
                            
                            # æ™ºèƒ½èåˆçŠ¶æ€æŒ‡ç¤º
                            if avg_pref_reward > 0.01:
                                print(f"  âœ… Episodeæ•´ä½“ç¬¦åˆåå¥½ (+{avg_pref_reward:.4f})")
                            elif avg_pref_reward < -0.01:
                                print(f"  âš ï¸ Episodeæ•´ä½“åç¦»åå¥½ ({avg_pref_reward:.4f})")
                            else:
                                print(f"  â– Episodeåå¥½ä¿¡å·å¾®å¼±")
                            
                            # é‡ç½®episodeèåˆæ•°æ®
                            self._episode_fusion_data = {
                                'fusion_count': 0,
                                'total_env_reward': 0.0,
                                'total_pref_reward': 0.0,
                                'total_integrated_reward': 0.0,
                                'total_fusion_time': 0.0,
                                'positive_preference_count': 0,
                                'negative_preference_count': 0,
                                'avg_confidence': 0.0,
                                'avg_pref_weight': 0.0,
                                'avg_env_weight': 0.0
                            }
                    
                    # å®Œæˆepisodeæ•°æ®æ”¶é›†ï¼ˆåå¥½å­¦ä¹ ï¼‰
                    self._finalize_episode_data()
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºåå¥½æ¨¡å‹
                    self._check_and_create_preference_model()
                    
                    # è®°å½•åå¥½å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯  
                    self._log_preference_stats()
                    
                    # æ¸…ç†ç¼“å­˜
                    self._cleanup_cache_if_needed()
                    
                    # åŸæœ‰çš„episodeç»“æŸå¤„ç†ï¼ˆæ¢å¤åŸå§‹TD-MPC2æµç¨‹ï¼‰
                    
                    train_metrics.update(
                        episode_env_reward=episode_env_reward,
                        episode_success=info.get("success", False),
                    )
                    train_metrics.update(self.common_metrics())

                    results_metrics = {
                        'return': train_metrics['episode_env_reward'],
                        'episode_length': len(self._tds[1:]),
                        'success': train_metrics['episode_success'],
                        'success_subtasks': info.get('success_subtasks', []),
                        'step': self._step,
                    }
                
                    self.logger.log(train_metrics, "train")
                    self.logger.log(results_metrics, "results")
                    self._ep_idx = self.buffer.add(torch.cat(self._tds))

                # ç»“æŸä¸Šä¸€ä¸ªepisodeï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                if hasattr(self, '_step') and self._step > 0:
                    self._finalize_episode_data()
                    # æ›´æ–°episodeè®¡æ•°å™¨
                    self.episodes_completed += 1
                    print(f"[IntegratedPreferenceTrainer] Episode {self.episodes_completed} å®Œæˆ")
                
                obs = self.env.reset()[0]
                self._tds = [self.to_td(obs)]
                
                # å¼€å§‹æ–°çš„episode
                if self.prioritized_integrator:
                    try:
                        self.current_episode_id = self.prioritized_integrator.start_episode()
                        # é‡ç½®è½¨è¿¹çº§åˆ«çš„åå¥½ç»Ÿè®¡
                        self.prioritized_integrator.reset_trajectory_stats()
                        print(f"[IntegratedPreferenceTrainer] ğŸš€ å¼€å§‹æ–°episode: {self.current_episode_id}")
                    except Exception as e:
                        print(f"[WARNING] ä¼˜å…ˆçº§é›†æˆå™¨start_episodeå¤±è´¥: {e}")
                        self.current_episode_id = None
                
                # åˆå§‹åŒ–episodeå¥–åŠ±è®°å½•ï¼ˆæ¢å¤åŸå§‹TD-MPC2æµç¨‹ï¼‰
                self._episode_env_reward = 0.0
                if hasattr(self, '_episode_integrated_reward'):
                    self._episode_integrated_reward = 0.0

            # æ”¶é›†ç»éªŒ
            if self._step > self.cfg.seed_steps:
                action = self.agent.act(obs, t0=len(self._tds) == 1)
                
                # ç§»é™¤æ··åˆä»·å€¼ä¼°è®¡ç›¸å…³çš„æ—¥å¿—è¾“å‡ºï¼ˆæ¢å¤åŸå§‹TD-MPC2æµç¨‹ï¼‰
                # ä¸å†è®°å½•æ··åˆä»·å€¼ä¼°è®¡ä½¿ç”¨æƒ…å†µ
            else:
                action = self.env.rand_act()
            
            obs, reward, done, truncated, info = self.env.step(action)
            done = done or truncated
            
            # æ”¶é›†æ­¥éª¤æ•°æ®åˆ°åå¥½å­¦ä¹ ç³»ç»Ÿ
            self._collect_step_data(obs, action, reward, done)
            
            # === åŒè·¯å¾„å¥–åŠ±èåˆå¤„ç† ===
            final_reward = reward  # é»˜è®¤ä½¿ç”¨ç¯å¢ƒå¥–åŠ±
            preference_reward = 0.0
            confidence = 0.0
            integrated_reward = reward
            
            # å¦‚æœæ½œç©ºé—´åå¥½ç³»ç»Ÿå¯ç”¨ä¸”æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®
            if (self.preference_integrator and 
                len(self.current_episode_latent_states) > 0 and 
                len(self.current_episode_actions) > 0):
                
                try:
                    fusion_start_time = time.time()
                    
                    # è·å–å½“å‰åºåˆ—çš„æ½œç©ºé—´çŠ¶æ€å’ŒåŠ¨ä½œ
                    if len(self.current_episode_latent_states) >= 1:
                        # ä½¿ç”¨æœ€è¿‘çš„çŠ¶æ€åºåˆ—è¿›è¡Œåå¥½å¥–åŠ±è®¡ç®—
                        seq_len = min(20, len(self.current_episode_latent_states))  # ä½¿ç”¨æœ€è¿‘20æ­¥
                        recent_latent_states = torch.stack(self.current_episode_latent_states[-seq_len:])
                        recent_actions = torch.stack([torch.from_numpy(a).float() if isinstance(a, np.ndarray) else a.float() 
                                                    for a in self.current_episode_actions[-seq_len:]])
                        
                        # è®¡ç®—é›†æˆå¥–åŠ± - ä½¿ç”¨æœ€æ–°çš„çŠ¶æ€å’ŒåŠ¨ä½œ
                        latest_latent_state = recent_latent_states[-1]  # å–æœ€æ–°çš„çŠ¶æ€
                        latest_action = recent_actions[-1]  # å–æœ€æ–°çš„åŠ¨ä½œ
                        
                        # ç¡®ä¿å¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                        device = self.agent.device if hasattr(self.agent, 'device') else 'cuda' if torch.cuda.is_available() else 'cpu'
                        latest_latent_state = latest_latent_state.to(device)
                        latest_action = latest_action.to(device)
                        
                        reward_details = self.latent_preference_integrator.compute_integrated_reward(
                            latent_state=latest_latent_state,
                            action=latest_action,
                            environment_reward=reward
                        )
                        integrated_reward = reward_details['integrated_reward']
                        
                        # æå–è¯¦ç»†ä¿¡æ¯
                        preference_reward = reward_details['preference_reward']
                        confidence = reward_details['confidence']
                        final_reward = integrated_reward
                        
                        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                        self.preference_stats['latent_preference_computations'] += 1
                        self.preference_stats['reward_fusions'] += 1
                        self.preference_stats['total_preference_reward'] += preference_reward
                        self.preference_stats['total_environment_reward'] += reward
                        self.preference_stats['total_integrated_reward'] += integrated_reward
                        
                        # è®°å½•æ€§èƒ½
                        fusion_time = time.time() - fusion_start_time
                        self.performance_monitor['reward_fusion_time'].append(fusion_time)
                        
                        # ç´¯ç§¯episodeå†…çš„èåˆæ•°æ®ï¼Œåªåœ¨episodeç»“æŸæ—¶è¾“å‡ºæ±‡æ€»
                        if not hasattr(self, '_episode_fusion_data'):
                            self._episode_fusion_data = {
                                'fusion_count': 0,
                                'total_env_reward': 0.0,
                                'total_pref_reward': 0.0,
                                'total_integrated_reward': 0.0,
                                'total_fusion_time': 0.0,
                                'positive_preference_count': 0,
                                'negative_preference_count': 0,
                                'avg_confidence': 0.0,
                                'avg_pref_weight': 0.0,
                                'avg_env_weight': 0.0
                            }
                        
                        # ç´¯ç§¯æ•°æ®
                        self._episode_fusion_data['fusion_count'] += 1
                        self._episode_fusion_data['total_env_reward'] += reward
                        self._episode_fusion_data['total_pref_reward'] += preference_reward
                        self._episode_fusion_data['total_integrated_reward'] += integrated_reward
                        self._episode_fusion_data['total_fusion_time'] += fusion_time
                        self._episode_fusion_data['avg_confidence'] += confidence
                        self._episode_fusion_data['avg_pref_weight'] += reward_details['preference_weight']
                        self._episode_fusion_data['avg_env_weight'] += reward_details['environment_weight']
                        
                        # åå¥½åˆ†ç±»ç»Ÿè®¡ï¼ˆåŸºäºæ­£è´Ÿæ•°åˆ¤æ–­ï¼‰
                        # æ­£æ•°åå¥½å¥–åŠ±è®¡ä¸ºæ­£å‘åå¥½+1ï¼Œè´Ÿæ•°åå¥½å¥–åŠ±è®¡ä¸ºè´Ÿå‘åå¥½+1
                        if preference_reward > 0:
                            self._episode_fusion_data['positive_preference_count'] += 1
                        elif preference_reward < 0:
                            self._episode_fusion_data['negative_preference_count'] += 1
                        
                except Exception as e:
                    # å¥–åŠ±èåˆå¤±è´¥æ—¶å›é€€åˆ°ç¯å¢ƒå¥–åŠ±
                    # é™é»˜å¤„ç†èåˆå¤±è´¥ï¼Œä½¿ç”¨ç¯å¢ƒå¥–åŠ±
                    final_reward = reward
            
            # æ›´æ–°episodeå¥–åŠ±ç»Ÿè®¡
            self._episode_env_reward += reward  # ç¯å¢ƒå¥–åŠ±ç»Ÿè®¡
            if hasattr(self, '_episode_integrated_reward'):
                self._episode_integrated_reward += final_reward
            else:
                self._episode_integrated_reward = final_reward
            
            # ä½¿ç”¨èåˆåçš„å¥–åŠ±è¿›è¡Œè®­ç»ƒ
            self._tds.append(self.to_td(obs, action, final_reward))
            
            # æ›´æ–°æ­¥æ•°
            self._step += 1
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            save_freq = getattr(self.cfg, 'save_freq', 100000)
            save_agent = getattr(self.cfg, 'save_agent', True)
            if save_agent and self._step > 0 and self._step % save_freq == 0:
                checkpoint_path = os.path.join(self.cfg.work_dir, f'checkpoint_step_{self._step}.pt')
                try:
                    self.agent.save(checkpoint_path)
                    # åŒæ—¶ä¿å­˜ä¸€ä¸ªæœ€æ–°çš„æ£€æŸ¥ç‚¹
                    latest_path = os.path.join(self.cfg.work_dir, 'latest_checkpoint.pt')
                    self.agent.save(latest_path)
                except Exception as e:
                    pass  # é™é»˜å¤„ç†ä¿å­˜å¤±è´¥
            
            # è®­ç»ƒæ™ºèƒ½ä½“
            if self._step > self.cfg.seed_steps and len(self._tds) > 1:
                if hasattr(self.agent, 'update'):
                    _train_metrics = self.agent.update(self.buffer)
                    train_metrics.update(_train_metrics)
                
                # æ£€æŸ¥å¹¶è®­ç»ƒä¼˜å…ˆçº§åå¥½æ¨¡å‹ï¼ˆåŸºäºepisodeé¢‘ç‡æ§åˆ¶ï¼‰
                # åªåœ¨episodeç»“æŸæ—¶æ£€æŸ¥ï¼Œé¿å…æ¯ä¸ªè®­ç»ƒæ­¥éª¤éƒ½æ£€æŸ¥
                if self.prioritized_integrator and done:
                    try:
                        # è·å–è®­ç»ƒé¢‘ç‡é…ç½®
                        train_every_n_episodes = getattr(self.cfg, 'train_every_n_episodes', 10)
                        
                        # æ£€æŸ¥æ˜¯å¦åˆ°äº†è®­ç»ƒæ—¶æœº
                        episodes_since_last_training = self.episodes_completed - self.last_preference_training_episode
                        should_check_training = episodes_since_last_training >= train_every_n_episodes
                        
                        if should_check_training:
                            if self.prioritized_integrator.should_train_preference_model():
                                
                                # è·å–åå¥½æ¨¡å‹å®ä¾‹
                                preference_model = None
                                if self.preference_trainer and hasattr(self.preference_trainer, 'models') and len(self.preference_trainer.models) > 0:
                                    preference_model = self.preference_trainer.models[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹
                                    print(f"[IntegratedPreferenceTrainer] ä¼ é€’åå¥½æ¨¡å‹: {type(preference_model).__name__}")
                                else:
                                    print(f"[IntegratedPreferenceTrainer] âš ï¸ æœªæ‰¾åˆ°åå¥½æ¨¡å‹å®ä¾‹")
                                
                                preference_metrics = self.prioritized_integrator.train_preference_model(preference_model)
                                if preference_metrics:
                                    train_metrics.update(preference_metrics)
                                    print(f"[IntegratedPreferenceTrainer] âœ… ä¼˜å…ˆçº§åå¥½æ¨¡å‹è®­ç»ƒå®Œæˆ")
                                    # æ›´æ–°æœ€åè®­ç»ƒçš„episode
                                    self.last_preference_training_episode = self.episodes_completed
                            else:
                                print(f"[IntegratedPreferenceTrainer] Episode {self.episodes_completed}: è®­ç»ƒæ¡ä»¶æœªæ»¡è¶³ï¼Œè·³è¿‡åå¥½æ¨¡å‹è®­ç»ƒ")
                    except Exception as e:
                        print(f"[WARNING] ä¼˜å…ˆçº§åå¥½æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
        
        # è®­ç»ƒç»“æŸæ—¶ï¼Œç»“æŸæœ€åä¸€ä¸ªepisode
        if self.current_episode_id is not None:
            self._finalize_episode_data()
        
        # è®­ç»ƒå®Œæˆ
    
    def save(self, fp):
        """ä¿å­˜æ¨¡å‹ï¼ˆé‡å†™ä»¥åŒ…å«åå¥½å­¦ä¹ çŠ¶æ€ï¼‰"""
        # å¦‚æœçˆ¶ç±»æœ‰saveæ–¹æ³•åˆ™è°ƒç”¨ï¼Œå¦åˆ™è·³è¿‡
        if hasattr(super(), 'save'):
            super().save(fp)
        
        try:
            # ä¿å­˜åå¥½å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯åˆ°æ—¥å¿—
            if self.historical_preference_manager:
                stats = self.historical_preference_manager.get_stats()
                print(f"[IntegratedPreferenceTrainer] ä¿å­˜æ—¶çš„åå¥½å­¦ä¹ çŠ¶æ€:")
                print(f"  - ç¼“å­˜è½¨è¿¹æ•°: {stats.get('total_trajectories', 0)}")
                print(f"  - ç¼“å­˜åå¥½å¯¹æ•°: {stats.get('total_preference_pairs', 0)}")
                print(f"  - æ¨¡å‹ç‰ˆæœ¬: {stats.get('model_info', {}).get('version', 0) if stats.get('model_info') else 0}")
                print(f"  - å†…å­˜ä½¿ç”¨: {stats.get('memory_usage_estimate', 'N/A')}")
                
        except Exception as e:
            print(f"[WARNING] ä¿å­˜åå¥½å­¦ä¹ çŠ¶æ€å¤±è´¥: {e}")
    
    def eval(self):
        """è¯„ä¼°æ¨¡å¼ï¼ˆæ¢å¤åŸå§‹TD-MPC2æµç¨‹ï¼‰"""
        # ç›´æ¥è°ƒç”¨çˆ¶ç±»è¯„ä¼°ï¼Œä¸æ·»åŠ åå¥½ç›¸å…³æŒ‡æ ‡
        return super().eval()
    
    def get_preference_stats(self) -> Dict[str, Any]:
        """è·å–åå¥½å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯"""
        if self.historical_preference_manager is None:
            return {'preference_learning_enabled': False}
        
        try:
            manager_stats = self.historical_preference_manager.get_stats()
            combined_stats = {
                'preference_learning_enabled': True,
                **self.preference_stats,
                **manager_stats,
                'performance_metrics': {
                    'data_collection_times': list(self.performance_monitor['data_collection_time']),
                    'model_training_times': list(self.performance_monitor['model_training_time']),
                    'reward_fusion_times': list(self.performance_monitor.get('reward_fusion_time', [])),
                    'latent_preference_computation_times': list(self.performance_monitor.get('latent_preference_computation_time', [])),
                }
            }
            return combined_stats
            
        except Exception as e:
            print(f"[WARNING] è·å–åå¥½å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {'preference_learning_enabled': True, 'error': str(e)}

def create_integrated_preference_trainer(*args, **kwargs) -> IntegratedPreferenceTrainer:
    """åˆ›å»ºé›†æˆåå¥½å­¦ä¹ è®­ç»ƒå™¨çš„ä¾¿æ·å‡½æ•°"""
    return IntegratedPreferenceTrainer(*args, **kwargs)

# ç¤ºä¾‹ç”¨æ³•å’Œæµ‹è¯•
if __name__ == "__main__":
    print("IntegratedPreferenceTrainer - å†…å­˜ç¼“å­˜ç‰ˆæœ¬")
    print("è¯¥æ¨¡å—å·²ç§»é™¤æ‰€æœ‰æ–‡ä»¶IOæ“ä½œï¼Œä½¿ç”¨çº¯å†…å­˜ç¼“å­˜")
    print("ç‰¹ç‚¹:")
    print("1. é›¶æ–‡ä»¶IOå¼€é”€")
    print("2. é«˜æ€§èƒ½å†…å­˜ç¼“å­˜")
    print("3. è‡ªåŠ¨ç¼“å­˜ç®¡ç†")
    print("4. å®æ—¶ç»Ÿè®¡ç›‘æ§")
    print("5. æ— ç¼é›†æˆTD-MPC2è®­ç»ƒæµç¨‹")
