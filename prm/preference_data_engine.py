#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åå¥½æ•°æ®å¼•æ“ (Preference Data Engine)

å…¼å®¹æ€§æ¨¡å—ï¼šä¸ºç°æœ‰çš„ grpo æ¨¡å—æä¾›å‘åå…¼å®¹çš„æ¥å£ã€‚
å®é™…åŠŸèƒ½ç”± preference_labeling_engine.py æä¾›ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- æä¾›å‘åå…¼å®¹çš„APIæ¥å£
- è½¨è¿¹æ•°æ®åŒ…è£…å’Œå¤„ç†
- åå¥½æ ‡ç­¾ç”Ÿæˆçš„å…¼å®¹æ€§å°è£…
- è½¨è¿¹è´¨é‡è¯„ä¼°

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-01-11
ç‰ˆæœ¬ï¼š2.0.0
"""

import numpy as np
from typing import Dict, Any, Optional, Tuple, List, Union
import logging
import warnings
import time

# å¯¼å…¥å®é™…çš„å®ç°
from .preference_labeling_engine import (
    PreferenceLabelingEngine, 
    LabelType, 
    create_preference_labeling_engine
)
from .trajectory_metrics import TrajectoryMetrics

# å¯¼å…¥å·²å¼ƒç”¨çš„è§„åˆ™æ³¨å†Œè¡¨ï¼ˆä»…ç”¨äºå…¼å®¹æ€§ï¼‰
try:
    from .rule_registry import RULE_REGISTRY
except ImportError:
    # å¦‚æœrule_registryä¸å¯ç”¨ï¼Œä½¿ç”¨ç©ºå­—å…¸
    RULE_REGISTRY = {}

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ç‰ˆæœ¬ä¿¡æ¯
__version__ = "2.0.0"
__author__ = "AI Assistant"
__email__ = "assistant@ai.com"

class TrajectoryWrapper:
    """è½¨è¿¹åŒ…è£…å™¨ - å…¼å®¹æ€§ç±»
    
    ä¸ºç°æœ‰ä»£ç æä¾›ç»Ÿä¸€çš„è½¨è¿¹æ•°æ®æ¥å£ï¼Œæ”¯æŒå¤šç§æ•°æ®æ ¼å¼çš„è½¨è¿¹ã€‚
    
    Attributes:
        data: åŸå§‹è½¨è¿¹æ•°æ®å­—å…¸
        states: çŠ¶æ€åºåˆ— (obsçš„åˆ«å)
        actions: åŠ¨ä½œåºåˆ— (actionçš„åˆ«å)
        rewards: å¥–åŠ±åºåˆ— (rewardçš„åˆ«å)
        dones: ç»“æŸæ ‡å¿—åºåˆ— (doneçš„åˆ«å)
        length: è½¨è¿¹é•¿åº¦
        total_reward: æ€»å¥–åŠ±
    """
    
    def __init__(self, data: Dict[str, Any]):
        """åˆå§‹åŒ–è½¨è¿¹åŒ…è£…å™¨
        
        Args:
            data: è½¨è¿¹æ•°æ®å­—å…¸ï¼ŒåŒ…å« obs, action, reward, done ç­‰é”®
            
        Raises:
            ValueError: å½“æ•°æ®æ ¼å¼ä¸æ­£ç¡®æ—¶
        """
        if not isinstance(data, dict):
            raise ValueError("è½¨è¿¹æ•°æ®å¿…é¡»æ˜¯å­—å…¸æ ¼å¼")
            
        self.data = data.copy()  # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
        
        # è®¾ç½®æ ‡å‡†å±æ€§
        self.states = np.array(data.get('obs', [])) if data.get('obs') is not None else np.array([])
        self.actions = np.array(data.get('action', [])) if data.get('action') is not None else np.array([])
        self.rewards = np.array(data.get('reward', [])) if data.get('reward') is not None else np.array([])
        self.dones = np.array(data.get('done', [])) if data.get('done') is not None else np.array([])
        
        # å…¼å®¹æ€§å±æ€§
        self.obs = self.states
        self.action = self.actions
        self.reward = self.rewards
        self.done = self.dones
        
        # è®¡ç®—è½¨è¿¹é•¿åº¦
        self.length = len(self.states) if len(self.states) > 0 else 0
        
        # è®¡ç®—æ€»å¥–åŠ±
        self.total_reward = float(np.sum(self.rewards)) if len(self.rewards) > 0 else 0.0
        
        # éªŒè¯æ•°æ®ä¸€è‡´æ€§
        self._validate_data()
    
    def _validate_data(self) -> None:
        """éªŒè¯è½¨è¿¹æ•°æ®çš„ä¸€è‡´æ€§
        
        Raises:
            ValueError: å½“æ•°æ®ä¸ä¸€è‡´æ—¶
        """
        lengths = []
        if len(self.states) > 0:
            lengths.append(len(self.states))
        if len(self.actions) > 0:
            lengths.append(len(self.actions))
        if len(self.rewards) > 0:
            lengths.append(len(self.rewards))
        if len(self.dones) > 0:
            lengths.append(len(self.dones))
            
        if lengths and len(set(lengths)) > 1:
            logger.warning(f"è½¨è¿¹æ•°æ®é•¿åº¦ä¸ä¸€è‡´: {lengths}ï¼Œä½¿ç”¨æœ€å°é•¿åº¦")
            self.length = min(lengths)
        elif lengths:
            self.length = lengths[0]
        else:
            self.length = 0
    
    def __len__(self) -> int:
        """è¿”å›è½¨è¿¹é•¿åº¦"""
        return self.length
    
    def get_state(self, index: int) -> Optional[np.ndarray]:
        """è·å–æŒ‡å®šç´¢å¼•çš„çŠ¶æ€
        
        Args:
            index: çŠ¶æ€ç´¢å¼•
            
        Returns:
            çŠ¶æ€æ•°ç»„æˆ–None
        """
        if 0 <= index < len(self.states):
            return self.states[index]
        return None
    
    def get_action(self, index: int) -> Optional[np.ndarray]:
        """è·å–æŒ‡å®šç´¢å¼•çš„åŠ¨ä½œ
        
        Args:
            index: åŠ¨ä½œç´¢å¼•
            
        Returns:
            åŠ¨ä½œæ•°ç»„æˆ–None
        """
        if 0 <= index < len(self.actions):
            return self.actions[index]
        return None
    
    def get_reward(self, index: int) -> float:
        """è·å–æŒ‡å®šç´¢å¼•çš„å¥–åŠ±
        
        Args:
            index: å¥–åŠ±ç´¢å¼•
            
        Returns:
            å¥–åŠ±å€¼ï¼Œå¦‚æœç´¢å¼•æ— æ•ˆåˆ™è¿”å›0.0
        """
        if 0 <= index < len(self.rewards):
            return float(self.rewards[index])
        return 0.0
    
    def is_done(self, index: int) -> bool:
        """æ£€æŸ¥æŒ‡å®šç´¢å¼•æ˜¯å¦ç»“æŸ
        
        Args:
            index: æ£€æŸ¥ç´¢å¼•
            
        Returns:
            æ˜¯å¦ç»“æŸï¼Œå¦‚æœç´¢å¼•æ— æ•ˆåˆ™è¿”å›False
        """
        if 0 <= index < len(self.dones):
            return bool(self.dones[index])
        return False
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        
        Returns:
            è½¨è¿¹æ•°æ®å­—å…¸çš„å‰¯æœ¬
        """
        return self.data.copy()
    
    def __repr__(self) -> str:
        """è¿”å›è½¨è¿¹çš„å­—ç¬¦ä¸²è¡¨ç¤º"""
        return f"TrajectoryWrapper(length={self.length}, total_reward={self.total_reward:.3f})"

class PreferenceDataEngine:
    """åå¥½æ•°æ®å¼•æ“ - å…¼å®¹æ€§ç±»
    
    ä¸ºç°æœ‰çš„ grpo æ¨¡å—æä¾›å‘åå…¼å®¹çš„æ¥å£ï¼Œå®é™…åŠŸèƒ½ç”± PreferenceLabelingEngine æä¾›ã€‚
    
    è¿™ä¸ªç±»ä¸»è¦ç”¨äºä¿æŒå‘åå…¼å®¹æ€§ï¼Œæ–°é¡¹ç›®å»ºè®®ç›´æ¥ä½¿ç”¨ PreferenceLabelingEngineã€‚
    
    Attributes:
        task_name: ä»»åŠ¡åç§°
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        labeling_engine: åº•å±‚çš„ PreferenceLabelingEngine å®ä¾‹
        trajectory_metrics: è½¨è¿¹æŒ‡æ ‡è®¡ç®—å™¨
        stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    
    def __init__(self, task_name: Optional[str] = None, config_path: Optional[str] = None):
        """åˆå§‹åŒ–åå¥½æ•°æ®å¼•æ“
        
        Args:
            task_name: ä»»åŠ¡åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨"default"
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            
        Raises:
            FileNotFoundError: å½“é…ç½®æ–‡ä»¶ä¸å­˜åœ¨æ—¶
            ValueError: å½“é…ç½®å‚æ•°æ— æ•ˆæ—¶
        """
        try:
            self.task_name = task_name or "default"
            self.config_path = config_path
            
            # åˆ›å»ºå®é™…çš„æ ‡ç­¾å¼•æ“
            self.labeling_engine = create_preference_labeling_engine(
                task_name=self.task_name,
                config_path=self.config_path
            )
            
            # åˆ›å»ºè½¨è¿¹æŒ‡æ ‡è®¡ç®—å™¨
            self.trajectory_metrics = TrajectoryMetrics()
            
            # ç»Ÿè®¡ä¿¡æ¯
            self.stats = {
                'total_comparisons': 0,
                'successful_comparisons': 0,
                'failed_comparisons': 0,
                'preference_labels_generated': 0,
                'quality_evaluations': 0,
                'trajectory_comparisons': 0
            }
            
            logger.info(f"[åå¥½æ•°æ®å¼•æ“] åˆå§‹åŒ–å®Œæˆï¼Œä»»åŠ¡: {self.task_name} (å…¼å®¹æ€§æ¨¡å¼)")
            
        except Exception as e:
            logger.error(f"[åå¥½æ•°æ®å¼•æ“] åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def generate_preference_labels(self, 
                                 obs_a: np.ndarray, 
                                 act_a: np.ndarray,
                                 obs_b: np.ndarray, 
                                 act_b: np.ndarray,
                                 label_type: str = "rule_based",
                                 **kwargs) -> Tuple[float, Dict[str, Any]]:
        """ç”Ÿæˆåå¥½æ ‡ç­¾ - å…¼å®¹æ€§æ–¹æ³•
        
        Args:
            obs_a: è½¨è¿¹Açš„è§‚æµ‹åºåˆ—
            act_a: è½¨è¿¹Açš„åŠ¨ä½œåºåˆ—
            obs_b: è½¨è¿¹Bçš„è§‚æµ‹åºåˆ—
            act_b: è½¨è¿¹Bçš„åŠ¨ä½œåºåˆ—
            label_type: æ ‡ç­¾ç±»å‹ï¼Œæ”¯æŒ 'rule_based', 'dpo_binary', 'quality_based'
            **kwargs: é¢å¤–çš„é…ç½®å‚æ•°
            
        Returns:
            (preference_score, metadata): åå¥½åˆ†æ•°å’Œå…ƒæ•°æ®
            
        Raises:
            ValueError: å½“æ ‡ç­¾ç±»å‹ä¸æ”¯æŒæˆ–è¾“å…¥æ•°æ®æ— æ•ˆæ—¶
        """
        # éªŒè¯è¾“å…¥å‚æ•°
        if not isinstance(obs_a, np.ndarray) or not isinstance(act_a, np.ndarray):
            raise ValueError("è½¨è¿¹Açš„è§‚æµ‹å’ŒåŠ¨ä½œåºåˆ—å¿…é¡»æ˜¯numpyæ•°ç»„")
        if not isinstance(obs_b, np.ndarray) or not isinstance(act_b, np.ndarray):
            raise ValueError("è½¨è¿¹Bçš„è§‚æµ‹å’ŒåŠ¨ä½œåºåˆ—å¿…é¡»æ˜¯numpyæ•°ç»„")
        if label_type not in ["rule_based", "dpo_binary", "quality_based"]:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ‡ç­¾ç±»å‹: {label_type}ï¼Œæ”¯æŒçš„ç±»å‹: rule_based, dpo_binary, quality_based")
        
        try:
            # è½¬æ¢æ ‡ç­¾ç±»å‹
            if label_type == "rule_based":
                lt = LabelType.RULE_BASED
            elif label_type == "dpo_binary":
                lt = LabelType.DPO_BINARY
            elif label_type == "quality_based":
                lt = LabelType.QUALITY_BASED
            else:
                lt = LabelType.RULE_BASED
            
            # ä½¿ç”¨æ ‡ç­¾å¼•æ“ç”Ÿæˆæ ‡ç­¾
            label = self.labeling_engine.generate_preference_labels(
                obs_a, act_a, obs_b, act_b, lt, **kwargs
            )
            
            # æ„é€ å…¼å®¹çš„è¿”å›æ ¼å¼
            metadata = {
                'confidence': float(label.metadata.confidence) if label.metadata else 0.5,
                'label_type': label.metadata.label_type.value if label.metadata else label_type,
                'quality_score_a': float(label.metadata.quality_score_a) if label.metadata else 0.5,
                'quality_score_b': float(label.metadata.quality_score_b) if label.metadata else 0.5,
                'is_valid': bool(label.is_valid),
                'validation_errors': list(label.validation_errors) if label.validation_errors else [],
                'trajectory_lengths': [len(obs_a), len(obs_b)]
            }
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats['preference_labels_generated'] += 1
            if label.is_valid:
                self.stats['successful_comparisons'] += 1
            else:
                self.stats['failed_comparisons'] += 1
            self.stats['total_comparisons'] += 1
            
            return float(label.preference_score), metadata
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆåå¥½æ ‡ç­¾å¤±è´¥: {e}")
            self.stats['failed_comparisons'] += 1
            self.stats['total_comparisons'] += 1
            
            # è¿”å›é»˜è®¤å€¼
            return 0.5, {
                'confidence': 0.1,
                'label_type': label_type,
                'quality_score_a': 0.5,
                'quality_score_b': 0.5,
                'is_valid': False,
                'validation_errors': [str(e)],
                'trajectory_lengths': [len(obs_a) if isinstance(obs_a, np.ndarray) else 0, 
                                     len(obs_b) if isinstance(obs_b, np.ndarray) else 0]
            }
    
    def evaluate_trajectory_quality(self, 
                                   obs_seq: Union[np.ndarray, Dict[str, Any]], 
                                   act_seq: Optional[np.ndarray] = None,
                                   method: str = "comprehensive",
                                   **kwargs) -> Tuple[float, Dict[str, float]]:
        """è¯„ä¼°è½¨è¿¹è´¨é‡ - å…¼å®¹æ€§æ–¹æ³•
        
        Args:
            obs_seq: è§‚æµ‹åºåˆ—æˆ–è½¨è¿¹æ•°æ®å­—å…¸
            act_seq: åŠ¨ä½œåºåˆ—ï¼ˆå½“obs_seqä¸ºæ•°ç»„æ—¶å¿…éœ€ï¼‰
            method: è¯„ä¼°æ–¹æ³•ï¼Œæ”¯æŒ 'comprehensive', 'reward_based', 'success_based'
            **kwargs: é¢å¤–çš„è¯„ä¼°å‚æ•°
            
        Returns:
            (quality_score, feature_scores): è´¨é‡åˆ†æ•°å’Œç‰¹å¾åˆ†æ•°
            
        Raises:
            ValueError: å½“è¾“å…¥å‚æ•°æ— æ•ˆæ—¶
        """
        if method not in ["comprehensive", "reward_based", "success_based"]:
            logger.warning(f"æœªçŸ¥çš„è¯„ä¼°æ–¹æ³•: {method}ï¼Œä½¿ç”¨é»˜è®¤æ–¹æ³• 'comprehensive'")
            method = "comprehensive"
            
        try:
            # å¤„ç†ä¸åŒçš„è¾“å…¥æ ¼å¼
            if isinstance(obs_seq, dict):
                # è¾“å…¥æ˜¯è½¨è¿¹å­—å…¸
                trajectory_data = obs_seq
                obs_array = np.array(trajectory_data.get('obs', []))
                act_array = np.array(trajectory_data.get('action', []))
            elif isinstance(obs_seq, np.ndarray) and act_seq is not None:
                # è¾“å…¥æ˜¯åˆ†ç¦»çš„è§‚æµ‹å’ŒåŠ¨ä½œåºåˆ—
                obs_array = obs_seq
                act_array = act_seq
            else:
                raise ValueError("å¿…é¡»æä¾›è§‚æµ‹åºåˆ—å’ŒåŠ¨ä½œåºåˆ—ï¼Œæˆ–è€…æä¾›å®Œæ•´çš„è½¨è¿¹æ•°æ®å­—å…¸")
            
            if len(obs_array) == 0 or len(act_array) == 0:
                logger.warning("è½¨è¿¹æ•°æ®ä¸ºç©ºï¼Œè¿”å›é»˜è®¤è´¨é‡åˆ†æ•°")
                return 0.1, {}
            
            # ä½¿ç”¨è½¨è¿¹æŒ‡æ ‡è®¡ç®—å™¨
            quality_score, feature_scores = self.trajectory_metrics.evaluate_trajectory_quality(
                obs_array, act_array, method=method, **kwargs
            )
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats['quality_evaluations'] += 1
            
            # ç¡®ä¿è¿”å›å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
            quality_score = max(0.0, min(1.0, float(quality_score)))
            
            logger.debug(f"è½¨è¿¹è´¨é‡è¯„ä¼°å®Œæˆ: {quality_score:.3f} (æ–¹æ³•: {method})")
            return quality_score, feature_scores
            
        except Exception as e:
            logger.error(f"è¯„ä¼°è½¨è¿¹è´¨é‡å¤±è´¥: {e}")
            self.stats['quality_evaluations'] += 1  # ä»ç„¶è®¡å…¥ç»Ÿè®¡
            return 0.1, {}
    
    def compare_trajectories(self, 
                           traj_a: Union[TrajectoryWrapper, Dict[str, Any]], 
                           traj_b: Union[TrajectoryWrapper, Dict[str, Any]],
                           rule_name: Optional[str] = None,
                           method: str = "auto",
                           **kwargs) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸¤ä¸ªè½¨è¿¹ - å…¼å®¹æ€§æ–¹æ³•
        
        Args:
            traj_a: è½¨è¿¹Aï¼ˆTrajectoryWrapperæˆ–å­—å…¸ï¼‰
            traj_b: è½¨è¿¹Bï¼ˆTrajectoryWrapperæˆ–å­—å…¸ï¼‰
            rule_name: æ¯”è¾ƒè§„åˆ™åç§°
            method: æ¯”è¾ƒæ–¹æ³•ï¼Œæ”¯æŒ 'auto', 'rule_based', 'quality_based'
            **kwargs: é¢å¤–çš„æ¯”è¾ƒå‚æ•°
            
        Returns:
            æ¯”è¾ƒç»“æœå­—å…¸ï¼ŒåŒ…å«preference, confidence, methodç­‰å­—æ®µ
            
        Raises:
            ValueError: å½“è¾“å…¥å‚æ•°æ— æ•ˆæ—¶
        """
        # éªŒè¯è¾“å…¥å‚æ•°
        if method not in ["auto", "rule_based", "quality_based"]:
            logger.warning(f"æœªçŸ¥çš„æ¯”è¾ƒæ–¹æ³•: {method}ï¼Œä½¿ç”¨é»˜è®¤æ–¹æ³• 'auto'")
            method = "auto"
        
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯TrajectoryWrapperç±»å‹
            if isinstance(traj_a, dict):
                traj_a = TrajectoryWrapper(traj_a)
            if isinstance(traj_b, dict):
                traj_b = TrajectoryWrapper(traj_b)
                
            if not isinstance(traj_a, TrajectoryWrapper) or not isinstance(traj_b, TrajectoryWrapper):
                raise ValueError(f"è½¨è¿¹æ•°æ®å¿…é¡»æ˜¯TrajectoryWrapperæˆ–å­—å…¸æ ¼å¼ï¼Œå¾—åˆ°: {type(traj_a)}, {type(traj_b)}")
            
            if rule_name and rule_name in RULE_REGISTRY:
                # ä½¿ç”¨æŒ‡å®šçš„è§„åˆ™
                compare_func = RULE_REGISTRY[rule_name]
                better_traj, worse_traj = compare_func(traj_a, traj_b, None)  # goal=None
                
                if better_traj is not None:
                    preference = 0 if better_traj == traj_a else 1
                    confidence = 0.8  # è§„åˆ™æ¯”è¾ƒçš„é»˜è®¤ç½®ä¿¡åº¦
                else:
                    preference = -1  # æ— æ³•æ¯”è¾ƒ
                    confidence = 0.0
                
                result = {
                    'preference': preference,
                    'confidence': confidence,
                    'method': 'rule_based',
                    'rule_name': rule_name,
                    'trajectory_lengths': [len(traj_a), len(traj_b)]
                }
            else:
                # ä½¿ç”¨è´¨é‡è¯„ä¼°è¿›è¡Œæ¯”è¾ƒ
                quality_a, _ = self.evaluate_trajectory_quality(traj_a.states, traj_a.actions, **kwargs)
                quality_b, _ = self.evaluate_trajectory_quality(traj_b.states, traj_b.actions, **kwargs)
                
                if quality_a > quality_b + 0.05:  # 5% é˜ˆå€¼
                    preference = 0  # traj_aæ›´å¥½
                elif quality_b > quality_a + 0.05:
                    preference = 1  # traj_bæ›´å¥½
                else:
                    preference = -1  # æ— æ³•åŒºåˆ†
                
                confidence = float(abs(quality_a - quality_b))
                
                result = {
                    'preference': preference,
                    'confidence': confidence,
                    'method': 'quality_based',
                    'quality_scores': [float(quality_a), float(quality_b)],
                    'trajectory_lengths': [len(traj_a), len(traj_b)]
                }
            
            # æ·»åŠ é€šç”¨ä¿¡æ¯
            result.update({
                'timestamp': time.time(),
                'task_name': getattr(self, 'task_name', 'unknown')
            })
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats['trajectory_comparisons'] += 1
            self.stats['total_comparisons'] += 1
            if result.get('preference', -1) != -1:
                self.stats['successful_comparisons'] += 1
            else:
                self.stats['failed_comparisons'] += 1
            
            logger.debug(f"è½¨è¿¹æ¯”è¾ƒå®Œæˆ: preference={result['preference']}, confidence={result['confidence']:.3f}, method={result['method']}")
            return result
                    
        except Exception as e:
            logger.error(f"æ¯”è¾ƒè½¨è¿¹å¤±è´¥: {e}")
            self.stats['failed_comparisons'] += 1
            self.stats['total_comparisons'] += 1
            
            return {
                'preference': -1,
                'confidence': 0.0,
                'method': 'error',
                'error': str(e),
                'timestamp': time.time(),
                'trajectory_lengths': [0, 0]
            }
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯ - å…¼å®¹æ€§æ–¹æ³•
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«å„ç§æ“ä½œçš„è®¡æ•°å’ŒæˆåŠŸç‡
        """
        try:
            # é¦–å…ˆå°è¯•ä»åº•å±‚å¼•æ“è·å–ç»Ÿè®¡ä¿¡æ¯
            engine_stats = self.labeling_engine.get_statistics()
            
            # è®¡ç®—æˆåŠŸç‡
            total_ops = self.stats.get('total_comparisons', 0)
            success_rate = (self.stats.get('successful_comparisons', 0) / total_ops * 100) if total_ops > 0 else 0.0
            
            # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
            combined_stats = {
                'task_name': getattr(self, 'task_name', 'unknown'),
                'config_path': getattr(self, 'config_path', None),
                
                # åŸºæœ¬ç»Ÿè®¡
                'total_comparisons': self.stats.get('total_comparisons', 0),
                'successful_comparisons': self.stats.get('successful_comparisons', 0),
                'failed_comparisons': self.stats.get('failed_comparisons', 0),
                'success_rate_percent': round(success_rate, 2),
                
                # è¯¦ç»†ç»Ÿè®¡
                'preference_labels_generated': self.stats.get('preference_labels_generated', 0),
                'quality_evaluations': self.stats.get('quality_evaluations', 0),
                'trajectory_comparisons': self.stats.get('trajectory_comparisons', 0),
                
                # ç³»ç»Ÿä¿¡æ¯
                'api_rules_loaded': len(self.labeling_engine.api_rules) if hasattr(self.labeling_engine, 'api_rules') else 0,
                'engine_type': type(self.labeling_engine).__name__ if hasattr(self, 'labeling_engine') else 'unknown',
                'trajectory_metrics_available': hasattr(self, 'trajectory_metrics'),
                
                # æ—¶é—´æˆ³
                'last_updated': time.time()
            }
            
            # åˆå¹¶åº•å±‚å¼•æ“çš„ç»Ÿè®¡ä¿¡æ¯
            if isinstance(engine_stats, dict):
                combined_stats.update(engine_stats)
                
            return combined_stats
            
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                'task_name': getattr(self, 'task_name', 'unknown'),
                'error': str(e),
                'last_updated': time.time()
            }
    
    def reset_statistics(self) -> None:
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total_comparisons': 0,
            'successful_comparisons': 0,
            'failed_comparisons': 0,
            'preference_labels_generated': 0,
            'quality_evaluations': 0,
            'trajectory_comparisons': 0
        }
        
        # ä¹Ÿé‡ç½®åº•å±‚å¼•æ“çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœæ”¯æŒï¼‰
        try:
            if hasattr(self.labeling_engine, 'reset_statistics'):
                self.labeling_engine.reset_statistics()
        except Exception as e:
            logger.warning(f"é‡ç½®åº•å±‚å¼•æ“ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            
        logger.info("ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
    
    def __repr__(self) -> str:
        """è¿”å›å¼•æ“çš„å­—ç¬¦ä¸²è¡¨ç¤º"""
        try:
            stats = self.get_statistics()
            return (f"PreferenceDataEngine(task='{stats.get('task_name', 'unknown')}', "
                    f"comparisons={stats.get('total_comparisons', 0)}, "
                    f"success_rate={stats.get('success_rate_percent', 0):.1f}%)")
        except:
            return f"PreferenceDataEngine(task='{getattr(self, 'task_name', 'unknown')}')"
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œè®°å½•æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            if hasattr(self, 'stats') and self.stats.get('total_comparisons', 0) > 0:
                stats = self.get_statistics()
                logger.info(f"PreferenceDataEngine é”€æ¯ï¼Œæœ€ç»ˆç»Ÿè®¡: {stats.get('total_comparisons', 0)} æ¬¡æ¯”è¾ƒï¼Œ"
                           f"æˆåŠŸç‡ {stats.get('success_rate_percent', 0):.1f}%")
        except:
            pass  # å¿½ç•¥ææ„æ—¶çš„é”™è¯¯

# å…¨å±€æ¯”è¾ƒè§„åˆ™å­—å…¸ - å…¼å®¹æ€§å˜é‡
global_compare_rules = RULE_REGISTRY.copy() if RULE_REGISTRY else {}

# å…¼å®¹æ€§å‡½æ•°
def global_compare_rules() -> Dict[str, Any]:
    """è·å–å…¨å±€æ¯”è¾ƒè§„åˆ™ - å…¼å®¹æ€§å‡½æ•°
    
    Returns:
        è§„åˆ™æ³¨å†Œè¡¨çš„å‰¯æœ¬
        
    Warning:
        æ­¤å‡½æ•°å·²å¼ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨ PreferenceLabelingEngine.api_rules
    """
    warnings.warn(
        "global_compare_rules() å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ PreferenceLabelingEngine.api_rulesã€‚"
        "æ–°ç³»ç»Ÿæä¾›æ›´å¥½çš„æ€§èƒ½å’Œæ›´å¤šåŠŸèƒ½ã€‚",
        DeprecationWarning,
        stacklevel=2
    )
    try:
        return RULE_REGISTRY.copy()
    except Exception as e:
        logger.error(f"è·å–å…¨å±€æ¯”è¾ƒè§„åˆ™å¤±è´¥: {e}")
        return {}

def auto_register_rules_for_task(task_name: str) -> bool:
    """ä¸ºæŒ‡å®šä»»åŠ¡è‡ªåŠ¨æ³¨å†Œè§„åˆ™ - å…¼å®¹æ€§å‡½æ•°
    
    Args:
        task_name: ä»»åŠ¡åç§°
        
    Returns:
        æ˜¯å¦æˆåŠŸæ³¨å†Œè§„åˆ™
        
    Warning:
        æ­¤å‡½æ•°å·²å¼ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨ PreferenceLabelingEngine
    """
    warnings.warn(
        "auto_register_rules_for_task() å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ PreferenceLabelingEngineã€‚"
        "æ–°ç³»ç»Ÿæä¾›è‡ªåŠ¨è§„åˆ™å‘ç°å’Œæ›´å¥½çš„ä»»åŠ¡é€‚é…ã€‚",
        DeprecationWarning,
        stacklevel=2
    )
    
    if not isinstance(task_name, str) or not task_name.strip():
        logger.warning("ä»»åŠ¡åç§°æ— æ•ˆï¼Œè¿”å›False")
        return False
    
    try:
        # æ£€æŸ¥è§„åˆ™æ³¨å†Œè¡¨æ˜¯å¦å·²æœ‰è§„åˆ™
        if len(RULE_REGISTRY) > 0:
            logger.info(f"[è§„åˆ™æ³¨å†Œ] ä»»åŠ¡ {task_name} çš„è§„åˆ™å·²æ³¨å†Œï¼Œå…± {len(RULE_REGISTRY)} ä¸ªè§„åˆ™")
            return True
        else:
            logger.warning(f"[è§„åˆ™æ³¨å†Œ] ä»»åŠ¡ {task_name} æ²¡æœ‰å¯ç”¨çš„è§„åˆ™")
            return False
    except Exception as e:
        logger.error(f"[è§„åˆ™æ³¨å†Œ] ä¸ºä»»åŠ¡ {task_name} æ³¨å†Œè§„åˆ™å¤±è´¥: {e}")
        return False

# å·¥å‚å‡½æ•° - å…¼å®¹æ€§å‡½æ•°
def create_preference_data_engine(task_name: Optional[str] = None, 
                                config_path: Optional[str] = None,
                                **kwargs) -> PreferenceDataEngine:
    """åˆ›å»ºåå¥½æ•°æ®å¼•æ“ - å…¼å®¹æ€§å‡½æ•°
    
    Args:
        task_name: ä»»åŠ¡åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨"default"
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        **kwargs: é¢å¤–çš„é…ç½®å‚æ•°
        
    Returns:
        PreferenceDataEngineå®ä¾‹
        
    Raises:
        ValueError: å½“å‚æ•°æ— æ•ˆæ—¶
        FileNotFoundError: å½“é…ç½®æ–‡ä»¶ä¸å­˜åœ¨æ—¶
    """
    try:
        return PreferenceDataEngine(task_name=task_name, config_path=config_path)
    except Exception as e:
        logger.error(f"åˆ›å»ºåå¥½æ•°æ®å¼•æ“å¤±è´¥: {e}")
        raise

def create_trajectory_wrapper(obs_seq: np.ndarray, 
                            act_seq: np.ndarray, 
                            rewards: Optional[np.ndarray] = None,
                            dones: Optional[np.ndarray] = None) -> TrajectoryWrapper:
    """åˆ›å»ºè½¨è¿¹åŒ…è£…å™¨ - å…¼å®¹æ€§å‡½æ•°
    
    Args:
        obs_seq: è§‚æµ‹åºåˆ—
        act_seq: åŠ¨ä½œåºåˆ—
        rewards: å¥–åŠ±åºåˆ—ï¼ˆå¯é€‰ï¼‰
        dones: ç»“æŸæ ‡å¿—åºåˆ—ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        è½¨è¿¹åŒ…è£…å™¨å®ä¾‹
        
    Raises:
        ValueError: å½“è¾“å…¥æ•°æ®æ ¼å¼æ— æ•ˆæ—¶
    """
    if not isinstance(obs_seq, np.ndarray) or not isinstance(act_seq, np.ndarray):
        raise ValueError(f"è§‚æµ‹å’ŒåŠ¨ä½œåºåˆ—å¿…é¡»æ˜¯numpyæ•°ç»„ï¼Œå¾—åˆ°: {type(obs_seq)}, {type(act_seq)}")
    
    if len(obs_seq) == 0 or len(act_seq) == 0:
        raise ValueError("è§‚æµ‹å’ŒåŠ¨ä½œåºåˆ—ä¸èƒ½ä¸ºç©º")
    
    try:
        data = {
            'obs': obs_seq,
            'action': act_seq,
            'reward': rewards if rewards is not None else np.zeros(len(obs_seq)),
            'done': dones if dones is not None else np.zeros(len(obs_seq), dtype=bool)
        }
        return TrajectoryWrapper(data)
    except Exception as e:
        logger.error(f"åˆ›å»ºè½¨è¿¹åŒ…è£…å™¨å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç  - å…¼å®¹æ€§éªŒè¯
    import sys
    import traceback
    
    def test_basic_functionality():
        """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
        print("=" * 60)
        print("æµ‹è¯•åå¥½æ•°æ®å¼•æ“åŸºæœ¬åŠŸèƒ½...")
        print("=" * 60)
        
        try:
            # åˆ›å»ºå¼•æ“
            print("1. åˆ›å»ºåå¥½æ•°æ®å¼•æ“...")
            engine = create_preference_data_engine("test_task")
            print(f"   âœ“ å¼•æ“åˆ›å»ºæˆåŠŸ: {engine}")
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            print("2. ç”Ÿæˆæµ‹è¯•è½¨è¿¹æ•°æ®...")
            obs_a = np.random.randn(50, 151)
            act_a = np.random.randn(50, 61)
            obs_b = np.random.randn(45, 151)
            act_b = np.random.randn(45, 61)
            print(f"   âœ“ è½¨è¿¹A: obs{obs_a.shape}, act{act_a.shape}")
            print(f"   âœ“ è½¨è¿¹B: obs{obs_b.shape}, act{act_b.shape}")
            
            # æµ‹è¯•è½¨è¿¹åŒ…è£…å™¨
            print("3. æµ‹è¯•è½¨è¿¹åŒ…è£…å™¨...")
            traj_wrapper_a = create_trajectory_wrapper(obs_a, act_a)
            traj_wrapper_b = create_trajectory_wrapper(obs_b, act_b)
            print(f"   âœ“ è½¨è¿¹AåŒ…è£…å™¨: {traj_wrapper_a}")
            print(f"   âœ“ è½¨è¿¹BåŒ…è£…å™¨: {traj_wrapper_b}")
            
            # æµ‹è¯•è´¨é‡è¯„ä¼°
            print("4. æµ‹è¯•è½¨è¿¹è´¨é‡è¯„ä¼°...")
            quality_a, features_a = engine.evaluate_trajectory_quality(obs_a, act_a)
            quality_b, features_b = engine.evaluate_trajectory_quality(obs_b, act_b)
            print(f"   âœ“ è½¨è¿¹Aè´¨é‡: {quality_a:.3f}")
            print(f"   âœ“ è½¨è¿¹Bè´¨é‡: {quality_b:.3f}")
            
            # æµ‹è¯•è½¨è¿¹æ¯”è¾ƒ
            print("5. æµ‹è¯•è½¨è¿¹æ¯”è¾ƒ...")
            traj_dict_a = {'obs': obs_a, 'action': act_a}
            traj_dict_b = {'obs': obs_b, 'action': act_b}
            comparison_result = engine.compare_trajectories(traj_dict_a, traj_dict_b)
            print(f"   âœ“ æ¯”è¾ƒç»“æœ: preference={comparison_result['preference']}, "
                  f"confidence={comparison_result['confidence']:.3f}, "
                  f"method={comparison_result['method']}")
            
            # æµ‹è¯•åå¥½æ ‡ç­¾ç”Ÿæˆ
            print("6. æµ‹è¯•åå¥½æ ‡ç­¾ç”Ÿæˆ...")
            trajectory_pairs = [(traj_dict_a, traj_dict_b)]
            labels = engine.generate_preference_labels(trajectory_pairs, "quality_based")
            if labels:
                label = labels[0]
                print(f"   âœ“ åå¥½æ ‡ç­¾: preference={label['preference']}, "
                      f"confidence={label['confidence']:.3f}, "
                      f"type={label['label_type']}")
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            print("7. è·å–ç»Ÿè®¡ä¿¡æ¯...")
            stats = engine.get_statistics()
            print(f"   âœ“ ç»Ÿè®¡ä¿¡æ¯: {stats['total_comparisons']} æ¬¡æ¯”è¾ƒ, "
                  f"æˆåŠŸç‡ {stats['success_rate_percent']:.1f}%")
            
            print("\n" + "=" * 60)
            print("âœ“ æ‰€æœ‰åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
            print("=" * 60)
            return True
            
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
            print("é”™è¯¯è¯¦æƒ…:")
            traceback.print_exc()
            return False
    
    def test_compatibility():
        """æµ‹è¯•å…¼å®¹æ€§åŠŸèƒ½"""
        print("\n" + "=" * 60)
        print("æµ‹è¯•å…¼å®¹æ€§åŠŸèƒ½...")
        print("=" * 60)
        
        try:
            # æµ‹è¯•å…¨å±€è§„åˆ™è·å–
            print("1. æµ‹è¯•å…¨å±€è§„åˆ™è·å–...")
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", DeprecationWarning)
                rules = global_compare_rules()
            print(f"   âœ“ è·å–åˆ° {len(rules)} ä¸ªè§„åˆ™")
            
            # æµ‹è¯•ä»»åŠ¡è§„åˆ™æ³¨å†Œ
            print("2. æµ‹è¯•ä»»åŠ¡è§„åˆ™æ³¨å†Œ...")
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", DeprecationWarning)
                success = auto_register_rules_for_task("test_task")
            print(f"   âœ“ ä»»åŠ¡è§„åˆ™æ³¨å†Œ: {success}")
            
            print("\n" + "=" * 60)
            print("âœ“ å…¼å®¹æ€§åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
            print("=" * 60)
            return True
            
        except Exception as e:
            print(f"\nâŒ å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
            print("é”™è¯¯è¯¦æƒ…:")
            traceback.print_exc()
            return False
    
    # è¿è¡Œæµ‹è¯•
    print("å¼€å§‹åå¥½æ•°æ®å¼•æ“æµ‹è¯•å¥—ä»¶...")
    
    success_basic = test_basic_functionality()
    success_compat = test_compatibility()
    
    if success_basic and success_compat:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åå¥½æ•°æ®å¼•æ“å·¥ä½œæ­£å¸¸ã€‚")
        sys.exit(0)
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ã€‚")
        sys.exit(1)